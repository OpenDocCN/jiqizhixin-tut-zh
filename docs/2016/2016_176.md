# 学界 | IBM 研究院和蒙特利尔大学合作论文：多尺度循环神经网络在对话生成中的应用

选自 arxiv.org

**作者：Iulian Vlad Serban、Yoshua Bengio 等人**

**机器之心编译**

**参与：mcgrady164**

![](img/efaa1f29ec8b34255569f7e651145e5f.jpg)

**摘要**

本文提出了一种多尺度循环神经网络模型，将自然语言生成的端到端框架扩展为两个并行的离散随机过程：一个是高层次的粗糙标记序列，一个是自然语言标记的序列。存在很多种估计或者学习高层次粗糙标记的方法，但我们采用了一种非常简单的提取方法有效地捕捉到了丰富的高层次语义。这种方法也支持在两个序列上最大化联合似然函数来训练多尺度循环神经网络模型。相对于标准似然目标函数影响自然语言标记（词混乱度），优化联合似然函数将会使得模型偏向于更高层次的建模。我们将本文模型应用到了两个具有挑战性的对话生成任务中：Ubuntu 技术支持和 Twitter 会话。在 Ubuntu 任务中，本文模型比其他模型具有更大的竞争力，不论是在自动评价指标上还是人工评价中都取得了最优的结果。在 Twitter 任务中，根据自动评价指标的反馈，本文模型生成了更加主题相关的对话。最后，我们的实验结果表明本文模型更加善于克服自然语言的稀疏性问题，而且更擅长捕捉长句结构。

**介绍**

循环神经网络以其在机器翻译、语音识别等任务中的出色表现在机器学习界越来越受欢迎。这些研究成果已经促使了一系列新的神经网络结构，包括注意力，记忆力和基于指针的机制。

先前的研究绝大多数都聚焦于用确定的端到端框架来开发新的神经网络结构。换言之，大家都聚焦于改变确定网络结构的参数来匹配输入序列和输出序列，然后用预测输出序列的最大似然概率来训练模型。不同的是，我们追求一个更加出色的研究，旨在将传统的端到端框架推广到多输入输出序列，这里的每个序列都有一个属于自己的随机过程。我们提出了一种新的循环神经网络模型，称为多尺度循环神经网络，该模型通过分解联合概率来获得多个并行的序列。特别地，我们构造了一种分层结构，将信息从高层次序列（摘要信息）流向低层次序列（比如，自然语言序列）。这种结构需要用大一种新的目标函数来作训练，即所有并行序列的联合似然函数（不同于单序列上的似然函数），着将会使得模型偏向于更高层次的建模。在测试的时候，模型先生成高层次序列，然后再生成自然语言序列。这种分层生成过程可以模拟复杂的、长程依赖的输出序列。

研究者们最近发现了在对话生成任务中应用端到端神经网络模型的关键问题。神经网络模型不能生成有意义的对话，是因为其无法将对话的上下文考虑在内，这也说明该类模型无法学习到有用的高层次摘要信息。基于之前模型的缺点，我们将本文模型应用到了两个具有挑战性的对话生成任务中：以目标为导向的 Ubuntu 技术支持和非目标导向的 Twitter 会话。在两个任务中均取得了有竞争力的结果。特别是在 Ubuntu 任务中，本文模型在人工评价和自动评价指标上都远胜于其他模型。

***©本文由机器之心编译，***转载请联系本公众号获得授权***。***

✄------------------------------------------------

**加入机器之心（全职记者/实习生）：hr@almosthuman.cn**

**投稿或寻求报道：editor@almosthuman.cn**

**广告&商务合作：bd@almosthuman.cn**