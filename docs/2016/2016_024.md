# 现场报道｜Geoffrey Hinton 最新演讲梳理，从人工神经网络到 RNN 应用

机器之心原创

**作者：Joshua Chou**

**翻译：吴攀、李亚洲**

**Ⅰ. 介绍**

Geoffrey Hinton 教授上周做了一次非常棒的演讲，我学到了很多。尽管该演讲是面向普通听众，但里面的好东西要比我预期的多，非常激动能听到 Hinton 教授分享他的研究。也就是说，那些很了解深度学习的人可以认为这是将 Hinton 的想法解释给对此领域了解不多的人。

**Ⅱ. 人工神经元和网络**

首先，让我们先讲讲什么是人工神经元。我们对大脑中神经元如何运作进行了粗野的理想化实现，以至于通过一堆这样的神经元的协作让我们能学习这类计算方式。粗简化实现就是神经元获得一些加权输入，它从神经元突触获得一些峰值电压（动作电位）。如果该输入超过特定阈值，它就会产生与输入线性相关的输出。这被称为修正线性神经元（rectified linear neuron）。我们可以在一个网络中连接这些神经元，并训练突触强点（权重）让这些网络完成一些任务。

让我们举例说明一下这种网络能做什么。假设我采用一张依照像素强度（pixel intensity）描述的照片（每个输入是一个像素的 3 元组 RGB 值），并假设这张图像中有一百万个像素。我们把此作为输入，输出就是一串描述图像是什么的单词。这是一项难以编写的计算机程序，也没人真的知道如何编写。有人花费了超过 50 年试图写出这样的程序，我敢打赌他们并没能走多远。在机器学习领域，可以说难以做编程，所以我们打算训练人工神经网络（ANN）做这件事。

人工神经网络就是一堆这样简单的神经元连接到一起。ANN 的最简单形式被称为前馈网络，该网络的思路是你采用如上所举例的图像像素作为输入，有多层神经元对应大脑不同的皮层区，而且你还有一些输出神经元给你回应，给出的回应对应不同的类。相邻层所有神经元之间的连接强度就是权重，学习内容包括调整神经元之间的这些权重。很明显，这样的东西能被用于做昂贵的计算，只要我们能搞清楚如何改变所有连接。

接下来，我将讲一种有效又易于理解的算法，但它也完全无用。你要做的就是采用一个 ANN，在权重上给它一些初始值。你用一张图像作为输入，而且你还知道正确的输出类该是什么，然后你观察输出。然后，你要做的就是采用训练案例的一个小的子集（典型的子集），在此网络上运行这些子集，看该网络怎么样。你拿出连接强度中的一个，并稍微的扰乱它（可把它认为是一个突变）。再次在网络上运行同样测试案例的子集，看它是变的更好还是更坏。如果变的更好，继续做变动。如果变的更坏，做相反的改动。这种方法有效但却非常非常慢。在此要重点记住的是，我们在实际网络中有千百万（或数十亿）的连接。为了升级一个连接的权重，上面的流程要在网络上对同样的子集进行成千上万次的运行。在效率上这种方法基本无望，但至少知道了它如何工作。为了更加有效，我将讲下反向传播算法。

**III. 反向传播算法**

反向传播算法是计算权重中的改变如何影响输出误差的一种有效方式。简单的说，不再是进行随机的突变，观察反向传播是否对此进行改进，网络观察输出和你给出答案（正确的或预期的输出）之间的差别。它通过使用网络中的权重搞清楚这种差别，并搞清楚如何改变这些连接能产生输出。这只是一些基础的微积分学。因为这些权重在网络中，权重中变化的关系和输出中发生了什么不是一个未知的秘密，但却是决定性的。这意味着你可以同时搞清楚所有的权重，要比朴素算法明显的更高效。

在反向传播算法中，你在网络中用现有的权重向前运行输入，观察网络输出与你想要输出之间的不同。然后将信息在一个反向通道中反向传送到网络，用来计算突触串（权重）中的一个小变化如何改变所得答案与正确答案之间的相差。并行的对每个连接进行这种工作，你就能朝着改进差异标准的方向轻微升级所有连接。

反向传播不是一直都很流行。它发明于 20 世纪 70 年代，但它没能像机器学习科学家预想的那样有效，因为有很多隐层的深度网络没有浅层网络好，在循环网络中也不很好。到了 90 年代，大部分人都放弃它了。这基本上就是反向传播的寒冬了。在加拿大的一些人开发出的技术使得机器学习在深度网络中表现更好，而后才渡过了寒冷的冬天。由于这不是该演讲的重点，我不会深入此话题。重点是通过开发这些技术，使得反向传播有效了，从广义上来说有更多的数据集和计算能力了，全世界都在追求反向传播了。

**IV. 循环神经网络**

循环神经网络（RNN）是该演讲的下一个主题。为了简洁，Hinton 教授略去了很多细节（我相信你可以很轻松地在网上找到相关材料）。本文也不会深入到相关细节中，而只会简单地描述一下这种网络。RNN 是一种接受输入序列并产生输出序列的的网络。这种网络由输入神经元、输出神经元和一些隐藏神经元组成。在每个时间步骤，隐藏神经元都会接收到一些状态，而隐藏神经元的状态依赖于它们在这个时间步骤所接受到的输入，也依赖于之间步骤的输入状态——这也是它们被称作「循环（recurrent）」的原因。关于 RNN 需要注意的一点是其权重在不同的时间步骤是捆绑在一起的，这样当它经过一系列的时间步骤时，它可以重复利用同样的权重。它们的训练方式在细节上和反向传播有一些相似，此处就不谈及这些细节了。了解更多信息，请参看 Hochreiter and Schmidhuber (1997) 了解目前效果最好的循环网络类型。

**V. 应用**

Hinton 教授谈了一些用 RNN 完成的一些应用。这些应用不管是新是旧，都能为被探索的可能的新领域提供见解（insights），并且在计算方法也取得了一些进步。

**i. 预测维基百科的下一个字符**

这是 Hinton 的组在 2010、2011 年完成的一项实验。将维基百科仅看作是一个字符的序列。该网络所见到的就是一个字符的序列——5 亿个字符，该网络要做的就是预测下一个字符。不同的字符有 86 个，输入就是这 86 个字符，而输出就是预测下一个会出现的字符是什么（86 个总和为 1 的概率）。正如我之前所提到的，我不会谈论算法上的细节。我很确定如果输入是「生命的意义是」，你会对这个训练出来的网络的输出感兴趣。下面这个输出结果仅供娱乐。

> *The meaning oflife is the tradition of the ancient human reproduction: it is less favorable to the good boy for when to remove her bigger.*

这可比 42 这个答案要有趣多了（42, 是《银河系漫游指南》一书里超级计算机对生命的意义的计算结果，非常富有喜剧性，是所有西方科技爱好者乐于引用的一个调侃型的典故。）!

**ii. 一种实现机器翻译的全新方法**

一个更激动人心的主题是相对较新的机器翻译应用，谷歌将其投入应用还不到两周。对于每一种语言，我们都有一个编码器 RNN 和一个解码器 RNN。其中编码器 RNN 获取一个词串（string of words），并以一些隐藏状态作为结束。我们将这些隐藏状态定义为「思想（thought）」。换句话说，任何词串都可以表达成一个思想（事实并不是听起来那么蠢）。英语编码器 RNN 的工作就是将一个英语词序列转换成一个思想向量（thought vector）。你训练这个 RNN 的方法是：你有成对的英语句子，使用比如法语的解码器 RNN 的反向传播，随着时间逐渐最大化产生特定翻译的概率。

在更容易的词中，你可以从完全随机的权重开始并馈送入英文句子的词，这些句子会通过这些随机的权重（随机思想）。该随机思想（random thought）被法语解码器 RNN 的随机权重编码以生成一个概率分布（输出）。我们从这个分布中选择词，并将其馈送回该 RNN 作为输入。给定其第一个词，该 RNN 可以为可能的第二个词特定一个分布。这个过程被不断重复完成，直到完成 full stop。本质上讲，我们想做的是获取输入串并确定其思想（thought）是什么，然后将其用另一种语言表达出来。显然，这就是机器翻译应该的工作方式。这种方法显著优于之前所用的词到词直接翻译的方法。

更多信息请参阅 Suskever, Vinyals, and Le, 2014

**VI. 处理噪杂数据的两种非常不同的方法**

该演讲的一项讨论是机器学习科学家的哲学与传统的数学家和统计学家处理嘈杂数据的方法之间的不同之处。传统的统计技术的目标是使用简单的模型表征数据，并且只相信不太可能是由噪声所引起的规律。这种方法是有漏洞的，我们只能检测到一些非常强的规律。但是，因为这些数学方法涉及到寻找变量和预测输出之间的关键，所以它们允许在模型参数上使用带有理论分析的的可追溯算法（traceable algorithm）。

另一方面，Hinton 教授相信，为了实现良好的表现，我们需要利用我们的计算能力，并向我们的网络中注入尽可能多的参数，从而获取该数据的所有规律（既有可靠的也有不可靠的），并结合所有它们的意见以做出预测。在经验上讲，这已经在完成许多任务上被证明是成功的了。还有其它解释、直觉知识、以及某种程度上的数学公式用于支持机器学习。但是，以我的意见看，尽管机器学习技术取得了成功，但或多或少还缺乏其基础的理论，甚至完全不存在。

**VII. 向前发展的机器学习**

正如你可能注意到的那样，上面的讨论全部围绕着通过监督学习训练的神经网络。而另一方面，无监督学习到目前为止还没有取得很大的成功，尽管无监督学习是对人类学习方式的一种更正确的描述。这无疑是一个雄心勃勃的目标，而许多研究者相信类似于人类的创造能力并不能通过完全的无监督学习机制被注入到机器中。这种怀疑不是没有意义的，因为在没有提供任何形式的监督的情况下，如果算法不知道数据中相关的变化，那么它将如何执行有意义的学习？

随着机器学习领域的继续扩张，它毫无疑问将会吸引到越来越多的人才并继续成熟。随着这一趋势的继续，也许我们最终将能发现机器学习应用的潜在理论，不管其是复杂神经网络的数学公式，还是输出解的优化证明，它肯定在学术研究方面都是一个激动人心的思想。

**VIII. 最后的想法**

因为机器学习领域是很有发展前途的，所以我认为我们应该记得机器学习的许多基本基础来自于传统的统计学和数学方法，尽管存在不同的哲学流派。因此，我们不应该忽视对这些基础学科的研究，因为它们能极大地加快我们理解机器学习技术的速度。

此外，我想提一下 Hinton 教授留下的一个开放的问题：

如果心智只是我们的大脑——一团神经元、突触、动作电位和神经递质构成的网络——的产物，那么我们可以将心智注入一个电脑建模了我们的大脑的人工神经网络吗？如果可以，这又意味着什么？

最后，作为结语。毫不奇怪，这个演讲厅已经爆满，我只能在后排的柱子旁边听 Hinton 教授的演讲，为了看到幻灯片，我时不时就要伸出脑袋去瞧一眼。听 Hinton 教授的演讲无疑是一种享受。这是一个高层次的演讲，但也透彻地介绍了机器学习领域及其应用，我希望能够通过这一篇评论将这些概念和知识传递给你。

******©本文由机器之心原创，***转载请联系本公众号获得授权******。***

✄------------------------------------------------

**加入机器之心（全职记者/实习生）：hr@almosthuman.cn**

**投稿或寻求报道：editor@almosthuman.cn**

**广告&商务合作：bd@almosthuman.cn**

![](img/a573ff7d72f49f8fe283857b964d06fd.jpg)