# 资源 | 如何通过 Python 打造一款简易版 AlphaGo?

选自 github

**机器之心编译**

**参与：吴攀**

> > *2017 年伊始[，再度出山的 AlphaGo 化名 Master 在网络围棋平台上打遍棋界无敌手](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650722021&idx=1&sn=ee24983b1fce5183bf33845efd367a3b&chksm=871b0a9bb06c838dd8e94363d0be6ec5ad28e46501b51ac951d998f03b83df8ea8c8ce37803a&scene=21#wechat_redirect)。你是否也想打造一个自己的 AlphaGo 呢？GitHub 用户 Brian Lee（brilee）发布了一个效仿 AlphaGo 的极简围棋引擎的 Python 实现 MuGo。*
> 
> MuGo 项目地址：https://github.com/brilee/MuGo
> 
> 这是 AlphaGo 基本组成部分的一个纯 Python 实现。
> 
> AlphaGo 逻辑/控制流（logic/control flow）其实并不非常复杂，本项目就是其复现。AlphaGo 的秘密武器是在于其各种神经网络之中。
> 
> 就我所知，AlphaGo 在对弈过程中使用了三个神经网络。
> 
> 第一个神经网络是一个速度很慢但很准确的策略网络（policy network）。这个网络被训练用来预测人类的走子（大约 57% 的准确度），它会输出一个可能走子的列表，并且每一种走子方式都对应了一个概率。这个网络为蒙特卡洛树搜索（MCTS）提供了可能的走子起点。这个神经网络很慢的一大原因是它具有很大的规模，这是因为这个神经网络的输入是围棋棋盘上的各种计算成本高昂的属性——气的数量、叫吃、征等等。
> 
> 第二个神经网络也是一个策略网络，它比第一个更小更快，但准确度更低（大约 24%），这个网络并不使用复杂的属性作为输入。一旦到达了当前 MCTS 树的叶节点（leaf node），这个第二个更快的网络就会被用来得到一个棋盘局面的可能走子，并且对这个这个最终局面进行评分。
> 
> 第三个神经网络是一个价值网络：它为棋盘输出一个预期获胜的范围，而不会自己下任何棋。
> 
> 然后，将使用第二个神经网络的蒙特卡洛得到的结果和使用第三个神经网络的价值计算结果进行平均，然后这个值被记为该 MCTS 节点的近似结果。
> 
> **开始**
> 
> **安装 TensorFlow**
> 
> 开始需要安装 TensorFlow，并使用 GPU 驱动器（即英伟达显卡的 CUDA 支持）
> 
> **获取用于监督学习的 SGF**
> 
> 接下来需要一个 SGF 文件源。你可以在 https://u-go.net/gamerecords 获取 15 年时长的高段位对局数据。你也可以从其它来源下载专业比赛的数据库。
> 
> **预处理 SGF**
> 
> 第三，对你的 SGF 文件进行预处理。这需要 SGF 文件中的所有局面并提取出每一个局面的特征以及记录正确的下一步走子。
> 
> 然后将这些局面分割成块（chunk）——一块用于测试，其它的都用于训练。这个步骤需要一定时间，而且要是你修改了 features.py 文件中的特征提取步骤，你还需要重新预处理。
> 
> > ***python main.py preprocess data/kgs-****
> 
> 注：这句代码用了通配符，比如说：KGS 目录可以名为 data/kgs-2006-01、data/kgs-2006-02 等等
> 
> **监督学习（策略网络）**
> 
> 使用上面预处理过的 SGF 数据（默认输出目录是 ./processed_data/），你可以训练策略网络。
> 
> > ***python main.py train processed_data/ --save-file=/tmp/***
> 
> 网络训练好了之后，当前模型会被保存在 --save-file。你可以通过如下代码继续训练这个网络：
> 
> > **python main.py train processed_data/ --read-file=/tmp/savedmodel**
> > 
> > ** --save-file=/tmp/savedmodel --epochs=10 --logdir=logs/my_training_run**
> 
> 此外，你也可以使用 TensorBoard 跟踪你的训练过程——如果你为每一次运行定义了一个不同的名字（如：logs/my_training_run、logs/my_training_run2），你可以将这些运行彼此重叠起来：
> 
> > ***tensorboard --logdir=logs/***
> 
> **与 MuGo 对弈**
> 
> MuGo 使用了 GTP 协议，你可以通过任何兼容 GTP 的程序来使用它。要调用原始策略网络，使用如下代码：
> 
> > ***python main.py gtp policy --read-file=/tmp/savedmodel***
> 
> 要调用集成了 MCTS 的策略网络版本，使用：
> 
> > ***python main.py gtp mcts --read-file=/tmp/savedmodel***
> 
> 通过 GTP 下棋的一种方式是使用 gogui-display（它有一个兼容 GTP 的 UI）。你可以在 http://gogui.sourceforge.net/ 下载 gogui 工具套件。参见 http://gogui.sourceforge.net/doc/reference-twogtp.html 了解使用 GTP 的有趣方式。
> 
> > ***gogui-twogtp -black 'python main.py gtp policy --read-file=/tmp/savedmodel' -white 'gogui-display' -size 19 -komi 7.5 -verbose -auto***
> 
> 另一种通过 GTP 玩的方式是对抗 GnuGo，同时还能观看比赛：
> 
> > ***BLACK="gnugo --mode gtp"***
> > 
> > ***WHITE="python main.py gtp policy --read-file=/tmp/savedmodel"***
> > 
> > ***TWOGTP="gogui-twogtp -black \"$BLACK\" -white \"$WHITE\" -games 10 \***
> > 
> > *** -size 19 -alternate -sgffile gnugo"***
> > 
> > ***gogui -size 19 -program "$TWOGTP" -computer-both -auto***
> 
> 还有一种玩法是通过 GTP 连接 CGOS（http://yss-aya.com/cgos/ 计算机围棋在线服务器）。由 boardspace.net 运营的 CGOS 服务器已经关闭了；你需要在 yss-aya.com 接入 CGOS 服务器。
> 
> 配置好了你的 cgos.config 文件之后，你可以通过 cgosGtp -c cgos.config 连接到 CGOS，以及使用 cgosView yss-aya.com 6819 查看你自己的游戏。
> 
> ***©本文由机器之心编译，***转载请联系本公众号获得授权***。***
> 
> ✄------------------------------------------------
> 
> **加入机器之心（全职记者/实习生）：hr@almosthuman.cn**
> 
> **投稿或寻求报道：editor@almosthuman.cn**
> 
> **广告&商务合作：bd@almosthuman.cn**