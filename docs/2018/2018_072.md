# 构建深度神经网络，我有 20 条「不成熟」的小建议

选自 PCC

**作者：Matt H、Daniel R**

**机器之心编译**

**参与：Geek ai、路**

> 本文介绍了构建深度神经网络的一些基本技巧，从通用技巧、神经网络调试和案例研究三方面展开。

在我们的机器学习实验室中，我们已经在许多高性能的机器上进行了成千上万个小时的训练，积累了丰富的经验。在这个过程中，并不只有电脑学习到了很多的知识，事实上我们研究人员也犯了很多错误，并且修复了很多漏洞。

在本文中，我们将根据自身经验（主要基于 TensorFlow）向大家提供一些训练深度神经网络的实用秘诀。有些建议可能对你来说可能已经很熟悉了，但是其他人可能并不太了解。另外还有些建议可能并不适用，甚至可能对于特定的任务来说是不好的建议，所以请谨慎使用！

这些都是一些广为人知的方法，我们也是站在了巨人的肩膀上！本文的目的只是高屋建瓴地对如何在实践中使用它们进行总结。

**通用秘诀**

使用 ADAM 优化器。它确实很有效，相对于较传统的优化器（如原版梯度下降），我们更喜欢使用 ADAM。在 TensorFlow 环境下使用 ADAM 时，请注意：如果你想要保存和恢复模型权重，请记住在设置完 AdamOptimizer 后设置 Saver，这是因为 ADAM 也有需要恢复的状态（即对应于每个权重的学习率）。

ReLU 是最好的非线性（激活函数），这就好比 Sublime 是最好的文本编辑器。但说实话，ReLU 确实是运行速度最快、最简便的，而且令人惊讶的是，它们在工作时梯度并不会逐渐减小（从而能够防止梯度消失）。尽管 sigmoid 是一个常用激活函数，但是它在 DNN 中传播梯度的效果并不太好。

不要在输出层使用激活函数。这应该是显而易见的，但是如果你通过一个共用的函数构建每一层，那这可能是一个很容易犯的错误：请确保在输出层不要使用激活函数。

为每一层添加一个偏置项。这是机器学习的入门知识：本质上，偏置项将一个平面转换到最佳拟合位置。在 y=mx+b 式中，b 是偏置项，使直线能够向上或向下移动到最佳的拟合位置。

使用方差缩放初始化。在 TensorFlow 中，该方法写作 tf.contrib.layers.variance_scaling_initializer()。根据我们的实验，这种初始化方法比常规高斯分布初始化、截断高斯分布初始化及 Xavier 初始化的泛化/缩放性能更好。粗略地说，方差缩放初始化根据每一层输入或输出的数量（在 TensorFlow 中默认为输入的数量）来调整初始随机权重的方差，从而帮助信号在不需要其他技巧（如梯度裁剪或批归一化）的情况下在网络中更深入地传播。Xavier 和方差缩放初始化类似，只不过 Xavier 中每一层的方差几乎是相同的；但是如果网络的各层之间规模差别很大（常见于卷积神经网络），则这些网络可能并不能很好地处理每一层中相同的方差。

白化（归一化）输入数据。在训练中，令样本点的值减去数据集的均值，然后除以它的标准差。当网络的权重在各个方向上延伸和扩展的程度越小，你的网络就能更快、更容易地学习。保持数据输入以均值为中心且方差不变有助于实现这一点。你还必须对每个测试输入也执行相同的归一化过程，所以请确保你的训练集与真实数据类似。

以合理地保留动态范围的方式对输入数据进行缩放。这个步骤和归一化有关，但是应该在归一化操作之前进行。例如，在真实世界中范围为 [0, 140000000] 的数据 x 通常可以用「tanh(x)」或「tanh(x/C)」来进行操作，其中 C 是某个常数，它可以对曲线进行拉伸，从而在 tanh 函数的动态倾斜（斜率较大）部分对更大输入范围内的数据进行拟合。尤其是在输入数据在函数的一端或者两端都不受限的时候，神经网络将在数据处于 (0,1) 时学习效果更好。

一般不要使用学习率衰减。在随机梯度下降（SGD）中，降低学习率是很常见的，但是 ADAM 天然地就考虑到了这个问题。如果你真的希望达到模型性能的极致，请在训练结束前的一小段时间内降低学习率；你可能会看到一个突然出现的很小的误差下降，之后它会再次趋于平缓。

如果你的卷积层有 64 或 128 个滤波器，这就已经足够了。特别是对于深度网络来说，比如 128 个滤波器就已经很多了。如果你已经拥有了大量的滤波器，那么再添加更多的滤波器可能并不会提升性能。

池化是为了变换不变性（transform invariance）。池化本质上是让网络学习到图像「某个部分」的「一般概念」。例如，最大池化能够帮助卷积网络对图像中特征的平移、旋转和缩放具备一定的鲁棒性。

**神经网络的调试**

如果网络学习效果很差（指网络在训练中的损失/准确率不收敛，或者你得不到想要的结果），你可以试试下面的这些秘诀：

过拟合！如果你的网络学习效果不佳，你首先应该做的就是去过拟合一个训练数据点。准确率基本上应该达到 100% 或 99.99%，或者说误差接近 0。如果你的神经网络不能对一个数据点达到过拟合，那么模型架构就可能存在很严重的问题，但这种问题可能是十分细微的。如果你可以过拟合一个数据点，但是在更大的集合上训练时仍然不能收敛，请尝试下面的几条建议。

降低学习率。你的网络会学习地更慢，但是它可能会找到一个之前使用较大的步长时没找到的最小值。（直观地说，你可以想象一下你正在走过路边的沟渠，此时你想要走进沟的最深处，在那里模型的误差是最小的。）

提高学习率。这将加快训练速度，有助于加强反馈回路（feedback loop）。这意味着你很快就能大概知道你的网络是否有效。尽管这样一来网络应该能更快地收敛，但是训练结果可能不会太好，而且这种「收敛」状态可能实际上是反复震荡的。（使用 ADAM 优化器时，我们认为在许多实验场景下，~0.001 是比较好的学习率。）

减小（小）批量处理的规模。将批处理大小减小到 1 可以向你提供与权重更新相关的更细粒度的反馈，你应该将该过程在 TensorBoard（或者其他的调试/可视化工具）中展示出来。

删掉批归一化层。在将批处理大小减小为 1 时，这样做会暴露是否有梯度消失和梯度爆炸等问题。我们曾经遇到过一个好几个星期都没有收敛的网络，当我们删除了批归一化层（BN 层）之后，我们才意识到第二次迭代的输出都是 NaN。在这里使用批量归一化层，相当于在需要止血带的伤口上贴上了创可贴。批归一化有它能够发挥效果的地方，但前提是你确定自己的网络没有 bug。

加大（小）批量处理的规模。使用一个更大的批处理规模——还觉得不够的话，如果可以，你不妨使用整个训练集——能减小梯度更新的方差，使每次迭代变得更加准确。换句话说，权重更新能够朝着正确的方向发展。但是！它的有效性存在上限，而且还有一些物理内存的限制。我们发现，这条建议通常不如前两个建议（将批处理规模减小到 1、删除批归一化层）有用。

检查你矩阵的重构「reshape」。大幅度的矩阵重构（比如改变图像的 X、Y 维度）会破坏空间局部性，使网络更不容易学习，因为这时网络也必须学习重构。（自然特征变得支离破碎。事实上自然特征呈现出空间局部性也是卷积神经网络能够如此有效的原因！）使用多个图像/通道进行重构时要特别小心；可以使用 numpy.stack() 进行适当的对齐操作。

仔细检查你的损失函数。如果我们使用的是一个复杂的函数，可以试着把它简化为 L1 或 L2 这样的形式。我们发现 L1 对异常值不那么敏感，当我们遇到带有噪声的批或训练点时，可以进行稍小幅度的调整。

如果可以，仔细检查你的可视化结果。你的可视化库（matplotlib、OpenCV 等）是否调整数据值的范围或是对它们进行裁剪？你可以考虑使用一种视觉上均匀的配色方案。

**案例研究**

为了使上文描述的过程更有关联性，下面给出了一些用于描述我们构建的卷积神经网络的部分真实回归实验的损失图（通过 TensorBoard 进行可视化）。

最初，网络完全没有学习：

![](img/cc88c7dde58a83b66799e005cc79eaac-fs8.png)

我们试着裁剪数据值，防止它们超越取值范围：

![](img/81c1dc5c11abdea81c8291014dc3aa1a-fs8.png)

看看这些没有经过平滑的值有多么「疯狂」！学习率太高了吗？我们试着降低学习率，并且在一组输入数据上进行训练：

![](img/9be93b479b4f4814d0c7fda0dce6da25-fs8.png)

你可以看到学习率最初的几个变化发生在哪里（大约训练了 300 步和 3000 步时）。显然，这里我们进行的学习率下降调整太快了。所以如果给它更长的学习率衰减时间，它将表现得更好（损失更低）：

![](img/7442978364b2ffb6468fc47a11bc64b1-fs8.png)

可以看到，学习率在第 2000 步和第 5000 步时下降。这种情况更好，但是仍然不够完美，因为损失并没有降到 0。

然后我们停止学习率衰减，并且尝试通过 tanh 函数将输入值移动到一个更狭窄的范围内。这很显然将误差值带到了 1 以下，但是我们始终不能过拟合训练集：

![](img/2a8cd251c077c8552dfda683e80732f5-fs8.png)

在这里我们发现了，通过删除批归一化层，网络很快地在一两次迭代之后输出 NaN。我们禁用了批归一化，并将初始化方法改为方差缩放法。这让一切都不一样了！我们可以过拟合仅仅包含一两个输入的测试集。然而，下面的图对 Y 轴进行了裁剪。初始误差值远远高于 5，这说明误差减小了近 4 个数量级：

![](img/aacf3db841c41f5685a7699e94dac3a4-fs8.png)

上方的图是非常平滑的，但是你可以看到，它极其迅速地过拟合了测试输入，并且随着时间推移，整个训练集的损失降到了 0.01 以下。这个过程没有降低学习率。之后，我们在学习率降低了一个数量级之后继续训练，得到了更好的结果：

![](img/cb3684d5c376daf8c2742b534a366a97-fs8.png)

这些结果要好得多！但是如果我们以几何级别降低学习率，而不是将训练分成两部分，会如何呢？

在每一步中将学习率乘以 0.9995，结果不是很好：

![](img/e6e92e322cdcfcbdb01f541b98a7d827-fs8.png)

这大概是因为学习率下降地太快了。乘数如果取 0.999995 会更好，但是结果和完全不衰减相差无几。我们从这个特定的实验序列中得出结论：批归一化隐藏了糟糕的初始化导致的梯度爆炸；并且除了在最后故意设计的一个学习率衰减可能有帮助，减小学习率对 ADAM 优化器并没有特别的帮助。与批归一化一样，对值进行裁剪掩盖了真正的问题。我们还通过 tanh 函数控制高方差的输入值。

我们希望这些基本的诀窍在你对构建深度神经网络更加熟悉的时候能够提供帮助。通常，正是简单的事情让一切变得不同。![](img/2d1c94eb4a4ba15f356c96c72092e02b-fs8.png)

 *当地时间 7 月 14 日，腾讯将在斯德哥尔摩举办 [Tencent Academic and Industrial Conference (TAIC)](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650744369&idx=5&sn=04d3111cbda45793743a91823f61162f&chksm=871ae24fb06d6b59e03fb2d7b5b5a8e62af0595421dc1800f56a88633a4b5a77ef28a6513c44&scene=21#wechat_redirect)，诚邀全球顶尖 AI 学者、青年研究员与腾讯七大事业群专家团队探讨最前沿 AI 研究与应用。点击阅读原文，参与报名。

![](img/e91ebe2408b197eab15d8f7a299ce9ef-fs8.png)*