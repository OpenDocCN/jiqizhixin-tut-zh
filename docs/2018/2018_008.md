# å…¥é—¨ | ä»é›¶å¼€å§‹ï¼Œäº†è§£å…ƒå­¦ä¹ 

é€‰è‡ª Medium

**ä½œè€…ï¼š****Thomas Wolf**

**æœºå™¨ä¹‹å¿ƒç¼–è¯‘**

**å‚ä¸ï¼šTianci LIUã€è·¯**

> æœ¬æ–‡ä»‹ç»äº†å…ƒå­¦ä¹ ï¼Œä¸€ä¸ªè§£å†³ã€Œå­¦ä¹ å¦‚ä½•å­¦ä¹ ã€çš„é—®é¢˜ã€‚

å…ƒå­¦ä¹ æ˜¯ç›®å‰æœºå™¨å­¦ä¹ é¢†åŸŸä¸€ä¸ªä»¤äººæŒ¯å¥‹çš„ç ”ç©¶è¶‹åŠ¿ï¼Œå®ƒè§£å†³çš„æ˜¯å­¦ä¹ å¦‚ä½•å­¦ä¹ çš„é—®é¢˜ã€‚

ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ ç ”ç©¶æ¨¡å¼æ˜¯ï¼šè·å–ç‰¹å®šä»»åŠ¡çš„å¤§å‹æ•°æ®é›†ï¼Œç„¶åç”¨è¿™ä¸ªæ•°æ®é›†ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚å¾ˆæ˜æ˜¾ï¼Œè¿™å’Œäººç±»åˆ©ç”¨ä»¥å¾€ç»éªŒï¼Œä»…ä»…é€šè¿‡å°‘é‡æ ·æœ¬å°±è¿…é€Ÿå®Œæˆå­¦ä¹ çš„æƒ…å†µç›¸å·®ç”šè¿œã€‚

å› ä¸ºäººç±»å­¦ä¹ äº†ã€Œå¦‚ä½•å­¦ä¹ ã€ã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†ä»ä¸€ä¸ªéå¸¸ç›´è§‚çš„å…ƒå­¦ä¹ ç®€ä»‹å…¥æ‰‹ï¼Œä»å®ƒæœ€æ—©çš„èµ·æºä¸€ç›´è°ˆåˆ°å¦‚ä»Šçš„å…ƒå­¦ä¹ ç ”ç©¶ç°çŠ¶ã€‚ç„¶åï¼Œæˆ‘ä¼šä»å¤´å¼€å§‹ï¼Œåœ¨ PyTorch ä¸­å®ç°ä¸€ä¸ªå…ƒå­¦ä¹ æ¨¡å‹ï¼ŒåŒæ—¶ä¼šåˆ†äº«ä¸€äº›ä»è¯¥é¡¹ç›®ä¸­å­¦åˆ°çš„ç»éªŒæ•™è®­ã€‚

**é¦–å…ˆï¼Œä»€ä¹ˆæ˜¯å­¦ä¹ ï¼Ÿ**

æˆ‘ä»¬å…ˆæ¥ç®€å•äº†è§£ä¸€ä¸‹ï¼Œå½“æˆ‘ä»¬è®­ç»ƒä¸€ä¸ªç”¨æ¥å®ç°çŒ«ç‹—å›¾åƒåˆ†ç±»çš„ç®€å•ç¥ç»ç½‘ç»œæ—¶ï¼Œåˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆã€‚å‡è®¾æˆ‘ä»¬ç°åœ¨æœ‰ä¸€å¼ çŒ«çš„å›¾åƒï¼Œä»¥åŠå¯¹åº”çš„è¡¨ç¤ºã€Œè¿™æ˜¯ä¸€åªçŒ«ã€çš„æ ‡ç­¾ã€‚ä¸ºç®€æ´èµ·è§ï¼Œæˆ‘åšäº†ä¸€ä¸ªç®€å•çš„åŠ¨ç”»æ¥å±•ç¤ºè®­ç»ƒçš„è¿‡ç¨‹ã€‚

![](img/c5da1705443eb5d216335969e4c3e357-fs8.png)

*ç¥ç»ç½‘ç»œè®­ç»ƒè¿‡ç¨‹çš„å•æ­¥ã€‚è¯¥ç½‘ç»œç”¨æ¥å®ç°çŒ«ç‹—å›¾åƒåˆ†ç±»ã€‚*

åå‘ä¼ æ’­æ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒä¸­å¾ˆå…³é”®çš„ä¸€æ­¥ã€‚å› ä¸ºç¥ç»ç½‘ç»œæ‰§è¡Œçš„è®¡ç®—å’ŒæŸå¤±å‡½æ•°éƒ½æ˜¯å¯å¾®å‡½æ•°ï¼Œå› æ­¤æˆ‘ä»¬èƒ½å¤Ÿæ±‚å‡ºç½‘ç»œä¸­æ¯ä¸€ä¸ªå‚æ•°æ‰€å¯¹åº”çš„æ¢¯åº¦ï¼Œè¿›è€Œå‡å°‘ç¥ç»ç½‘ç»œå½“å‰ç»™å‡ºçš„é¢„æµ‹æ ‡ç­¾ä¸çœŸå®/ç›®æ ‡æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ï¼ˆè¿™ä¸ªå·®å¼‚æ˜¯ç”¨æŸå¤±å‡½æ•°åº¦é‡çš„ï¼‰ã€‚åœ¨åå‘ä¼ æ’­å®Œæˆåï¼Œå°±å¯ä»¥ä½¿ç”¨ä¼˜åŒ–å™¨æ¥è®¡ç®—æ¨¡å‹çš„æ›´æ–°å‚æ•°äº†ã€‚è€Œè¿™æ­£æ˜¯ä½¿ç¥ç»ç½‘ç»œçš„è®­ç»ƒæ›´åƒæ˜¯ä¸€é—¨ã€Œè‰ºæœ¯ã€è€Œä¸æ˜¯ç§‘å­¦çš„åŸå› ï¼šå› ä¸ºæœ‰å¤ªå¤šçš„ä¼˜åŒ–å™¨å’Œä¼˜åŒ–è®¾ç½®ï¼ˆè¶…å‚æ•°ï¼‰å¯ä¾›é€‰æ‹©äº†ã€‚

æˆ‘ä»¬æŠŠè¯¥ã€Œå•ä¸ªè®­ç»ƒæ­¥ã€æ”¾åœ¨ä¸€å¼ å›¾ä¸­å±•ç¤ºï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

![](img/da94f45c51d4a3617ca7b88f36683020-fs8.png)

ç°åœ¨ï¼Œè®­ç»ƒå›¾åƒæ˜¯ä¸€åªğŸˆï¼Œè¡¨ç¤ºå›¾åƒæ˜¯ä¸€åªçŒ«çš„æ ‡ç­¾æ˜¯ ğŸ”ºã€‚æœ€å¤§çš„è¿™äº› â–³ è¡¨ç¤ºæˆ‘ä»¬çš„ç¥ç»ç½‘ç»œï¼Œé‡Œé¢çš„ â–  è¡¨ç¤ºå‚æ•°å’Œæ¢¯åº¦ï¼Œæ ‡æœ‰ L çš„å››è¾¹å½¢è¡¨ç¤ºæŸå¤±å‡½æ•°ï¼Œæ ‡æœ‰ O çš„å››è¾¹å½¢è¡¨ç¤ºä¼˜åŒ–å™¨ã€‚

å®Œæ•´çš„å­¦ä¹ è¿‡ç¨‹å°±æ˜¯ä¸æ–­åœ°é‡å¤è¿™ä¸ªä¼˜åŒ–æ­¥ï¼Œç›´åˆ°ç¥ç»ç½‘ç»œä¸­çš„å‚æ•°æ”¶æ•›åˆ°ä¸€ä¸ªä¸é”™çš„ç»“æœä¸Šã€‚

![](img/d6f15ddfd9c7be931c234f452ec1a717-fs8.png)

*ä¸Šå›¾è¡¨ç¤ºç¥ç»ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹çš„ä¸‰æ­¥ï¼Œç¥ç»ç½‘ç»œï¼ˆç”¨æœ€å¤§çš„ â–³ è¡¨ç¤ºï¼‰ç”¨äºå®ç°çŒ«ç‹—å›¾åƒåˆ†ç±»ã€‚*

**å…ƒå­¦ä¹ **

å…ƒå­¦ä¹ çš„æ€æƒ³æ˜¯å­¦ä¹ ã€Œå­¦ä¹ ï¼ˆè®­ç»ƒï¼‰ã€è¿‡ç¨‹ã€‚

å…ƒå­¦ä¹ æœ‰å¥½å‡ ç§å®ç°æ–¹æ³•ï¼Œä¸è¿‡æœ¬æ–‡è°ˆåˆ°çš„ä¸¤ç§ã€Œå­¦ä¹ ã€å­¦ä¹ ã€è¿‡ç¨‹ã€çš„æ–¹æ³•å’Œä¸Šæ–‡ä»‹ç»çš„æ–¹å¼å¾ˆç±»ä¼¼ã€‚

åœ¨æˆ‘ä»¬çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå…·ä½“è€Œè¨€ï¼Œå¯ä»¥å­¦ä¹ åˆ°ä¸¤ç‚¹ï¼š

![](img/458feffda221da9e9cba1dffd36de4fd-fs8.png)

*   ç¥ç»ç½‘ç»œçš„åˆå§‹å‚æ•°ï¼ˆå›¾ä¸­çš„è“è‰²â– ï¼‰ï¼›

*   ä¼˜åŒ–å™¨çš„å‚æ•°ï¼ˆç²‰è‰²çš„â˜…ï¼‰ã€‚

æˆ‘ä¼šä»‹ç»å°†è¿™ä¸¤ç‚¹ç»“åˆçš„æƒ…å†µï¼Œä¸è¿‡è¿™é‡Œçš„æ¯ä¸€ç‚¹æœ¬èº«ä¹Ÿéå¸¸æœ‰è¶£ï¼Œè€Œä¸”å¯è·å¾—åˆ°ç®€åŒ–ã€åŠ é€Ÿä»¥åŠä¸€äº›ä¸é”™çš„ç†è®ºç»“æœã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªéƒ¨åˆ†éœ€è¦è®­ç»ƒï¼š

*   ç”¨ã€Œæ¨¡å‹ï¼ˆMï¼‰ã€è¿™ä¸ªè¯æ¥æŒ‡ä»£æˆ‘ä»¬ä¹‹å‰çš„ç¥ç»ç½‘ç»œï¼Œç°åœ¨ä¹Ÿå¯ä»¥å°†å…¶ç†è§£ä¸ºä¸€ä¸ªä½çº§ç½‘ç»œã€‚æœ‰æ—¶ï¼Œäººä»¬ä¹Ÿä¼šç”¨ã€Œä¼˜åŒ–å¯¹è±¡ï¼ˆoptimizeeï¼‰ã€æˆ–è€…ã€Œå­¦ä¹ å™¨ï¼ˆlearnerï¼‰ã€æ¥ç§°å‘¼å®ƒã€‚è¯¥æ¨¡å‹çš„æƒé‡åœ¨å›¾ä¸­ç”¨ â–  è¡¨ç¤ºã€‚

*   ç”¨ã€Œä¼˜åŒ–å™¨ï¼ˆOï¼‰ã€æˆ–è€…ã€Œå…ƒå­¦ä¹ å™¨ã€æ¥æŒ‡ä»£ç”¨äºæ›´æ–°ä½çº§ç½‘ç»œï¼ˆå³ä¸Šè¿°æ¨¡å‹ï¼‰æƒé‡çš„é«˜çº§æ¨¡å‹ã€‚ä¼˜åŒ–å™¨çš„æƒé‡åœ¨å›¾ä¸­ç”¨ â˜… è¡¨ç¤ºã€‚

**å¦‚ä½•å­¦ä¹ è¿™äº›å…ƒå‚æ•°ï¼Ÿ**

äº‹å®ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å°†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å…ƒæŸå¤±çš„æ¢¯åº¦åå‘ä¼ æ’­åˆ°åˆå§‹çš„æ¨¡å‹æƒé‡å’Œ/æˆ–ä¼˜åŒ–å™¨çš„å‚æ•°ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰äº†ä¸¤ä¸ªåµŒå¥—çš„è®­ç»ƒè¿‡ç¨‹ï¼šä¼˜åŒ–å™¨/å…ƒå­¦ä¹ å™¨ä¸Šçš„å…ƒè®­ç»ƒè¿‡ç¨‹ï¼Œå…¶ä¸­ï¼ˆå…ƒï¼‰å‰å‘ä¼ è¾“åŒ…å«æ¨¡å‹çš„å¤šä¸ªè®­ç»ƒæ­¥ï¼šæˆ‘ä»¬ä¹‹å‰è§è¿‡çš„å‰é¦ˆã€åå‘ä¼ æ’­ä»¥åŠä¼˜åŒ–æ­¥éª¤ã€‚

ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹å…ƒè®­ç»ƒçš„æ­¥éª¤ï¼š

![](img/a573606d5c1f4ddbfbe475b283fdace9-fs8.png)

*å…ƒè®­ç»ƒæ­¥ï¼ˆè®­ç»ƒä¼˜åŒ–å™¨ Oï¼‰åŒ…å« 3 ä¸ªæ¨¡å‹ï¼ˆMï¼‰çš„è®­ç»ƒæ­¥ã€‚*

åœ¨è¿™é‡Œï¼Œå…ƒè®­ç»ƒè¿‡ç¨‹ä¸­çš„å•ä¸ªæ­¥éª¤æ˜¯æ¨ªå‘è¡¨ç¤ºçš„ã€‚å®ƒåŒ…å«æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸¤ä¸ªæ­¥éª¤ï¼ˆåœ¨å…ƒå‰é¦ˆå’Œå…ƒåå‘ä¼ æ’­çš„æ–¹æ ¼ä¸­çºµå‘è¡¨ç¤ºï¼‰ï¼Œæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹å’Œæˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„è®­ç»ƒè¿‡ç¨‹å®Œå…¨ä¸€æ ·ã€‚

å¯ä»¥çœ‹åˆ°ï¼Œå…ƒå‰å‘ä¼ è¾“çš„è¾“å…¥æ˜¯åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä¾æ¬¡ä½¿ç”¨çš„ä¸€åˆ—æ ·æœ¬/æ ‡ç­¾ï¼ˆæˆ–ä¸€åˆ—æ‰¹æ¬¡ï¼‰ã€‚

![](img/17ecfbbddc3f6ca5f57d43b4254bf4cf-fs8.png)

*å…ƒè®­ç»ƒæ­¥ä¸­çš„è¾“å…¥æ˜¯ä¸€åˆ—æ ·æœ¬ï¼ˆğŸˆã€ğŸ•ï¼‰åŠå…¶å¯¹åº”çš„æ ‡ç­¾ï¼ˆğŸ”ºã€ğŸ”»ï¼‰ã€‚*

æˆ‘ä»¬åº”è¯¥å¦‚ä½•ä½¿ç”¨å…ƒæŸå¤±æ¥è®­ç»ƒå…ƒå­¦ä¹ å™¨å‘¢ï¼Ÿåœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥å°†æ¨¡å‹çš„é¢„æµ‹å’Œç›®æ ‡æ ‡ç­¾åšæ¯”è¾ƒï¼Œå¾—åˆ°è¯¯å·®å€¼ã€‚

> åœ¨è®­ç»ƒå…ƒå­¦ä¹ å™¨æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å…ƒæŸå¤±æ¥åº¦é‡å…ƒå­¦ä¹ å™¨åœ¨ç›®æ ‡ä»»åŠ¡â€”â€”è®­ç»ƒæ¨¡å‹â€”â€”ä¸Šçš„è¡¨ç°ã€‚

ä¸€ä¸ªå¯è¡Œçš„æ–¹æ³•æ˜¯åœ¨ä¸€äº›è®­ç»ƒæ•°æ®ä¸Šè®¡ç®—æ¨¡å‹çš„æŸå¤±ï¼šæŸå¤±è¶Šä½ï¼Œæ¨¡å‹å°±è¶Šå¥½ã€‚æœ€åï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡ºå…ƒæŸå¤±ï¼Œæˆ–è€…ç›´æ¥å°†æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å·²ç»è®¡ç®—å¾—åˆ°çš„æŸå¤±ç»“åˆåœ¨ä¸€èµ·ï¼ˆä¾‹å¦‚ï¼ŒæŠŠå®ƒä»¬ç›´æ¥åŠ èµ·æ¥ï¼‰ã€‚

æˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ªå…ƒä¼˜åŒ–å™¨æ¥æ›´æ–°ä¼˜åŒ–å™¨çš„æƒé‡ï¼Œåœ¨è¿™é‡Œï¼Œé—®é¢˜å°±å˜å¾—å¾ˆã€Œmetaã€äº†ï¼šæˆ‘ä»¬å¯ä»¥ç”¨å¦ä¸€ä¸ªå…ƒå­¦ä¹ å™¨æ¥ä¼˜åŒ–å½“å‰çš„å…ƒå­¦ä¹ å™¨â€¦â€¦ä¸è¿‡æœ€ç»ˆï¼Œæˆ‘ä»¬éœ€è¦äººä¸ºé€‰æ‹©ä¸€ä¸ªä¼˜åŒ–å™¨ï¼Œä¾‹å¦‚ SGD æˆ–è€… ADAMï¼ˆä¸èƒ½åƒã€Œturtles all the way downã€ä¸€æ ·ï¼ˆæ³¨ï¼šturtles all the way down è¿™é‡Œå¤§æ¦‚æ˜¯è¯´ï¼Œã€Œä¸èƒ½ä¸€ä¸ªæ¨¡å‹å¥—ä¸€ä¸ªæ¨¡å‹ï¼Œè¿™æ ·æ— é™çš„å¥—ä¸‹å»ã€ï¼‰ã€‚

è¿™é‡Œç»™å‡ºä¸€äº›å¤‡æ³¨ï¼Œå®ƒä»¬å¯¹äºæˆ‘ä»¬ç°åœ¨è¦è®¨è®ºçš„å®ç°è€Œè¨€éå¸¸é‡è¦ï¼š

*   äºŒé˜¶å¯¼æ•°ï¼šå°†å…ƒæŸå¤±é€šè¿‡æ¨¡å‹çš„æ¢¯åº¦è¿›è¡Œåå‘ä¼ æ’­æ—¶ï¼Œéœ€è¦è®¡ç®—å¯¼æ•°çš„å¯¼æ•°ï¼Œä¹Ÿå°±æ˜¯äºŒé˜¶å¯¼æ•°ï¼ˆåœ¨æœ€åä¸€ä¸ªåŠ¨ç”»ä¸­çš„å…ƒåå‘ä¼ æ’­éƒ¨åˆ†ï¼Œè¿™æ˜¯ç”¨ç»¿è‰²çš„ â–² ç©¿è¿‡ç»¿è‰²çš„ â–  æ¥è¡¨ç¤ºçš„ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ TensorFlow æˆ– PyTorch ç­‰ç°ä»£æ¡†æ¶æ¥è®¡ç®—äºŒé˜¶å¯¼æ•°ï¼Œä¸è¿‡åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¸è€ƒè™‘äºŒé˜¶å¯¼æ•°ï¼Œè€Œåªæ˜¯é€šè¿‡æ¨¡å‹æƒé‡è¿›è¡Œåå‘ä¼ æ’­ï¼ˆå…ƒåå‘ä¼ æ’­å›¾ä¸­çš„é»„è‰² â– ï¼‰ï¼Œä»¥é™ä½å¤æ‚åº¦ã€‚

*   åæ ‡å…±äº«ï¼šå¦‚ä»Šï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­çš„å‚æ•°æ•°é‡éå¸¸å¤šï¼ˆåœ¨ NLP ä»»åŠ¡ä¸­ï¼Œå¾ˆå®¹æ˜“å°±æœ‰å°†è¿‘ 3000 ä¸‡ ï½ï¼’äº¿ä¸ªå‚æ•°ï¼‰ã€‚å½“å‰çš„ GPU å†…å­˜æ— æ³•å°†è¿™ä¹ˆå¤šå‚æ•°ä½œä¸ºå•ç‹¬è¾“å…¥ä¼ è¾“ç»™ä¼˜åŒ–å™¨ã€‚æˆ‘ä»¬ç»å¸¸é‡‡ç”¨çš„æ–¹æ³•æ˜¯ã€Œåæ ‡å…±äº«ã€ï¼ˆcoordinate sharingï¼‰ï¼Œè¿™è¡¨ç¤ºæˆ‘ä»¬ä¸ºä¸€ä¸ªå‚æ•°è®¾è®¡ä¸€ä¸ªä¼˜åŒ–å™¨ï¼Œç„¶åå°†å…¶å¤åˆ¶åˆ°æ‰€æœ‰çš„å‚æ•°ä¸Šï¼ˆå…·ä½“è€Œè¨€ï¼Œå°†å®ƒçš„æƒé‡æ²¿ç€æ¨¡å‹å‚æ•°çš„è¾“å…¥ç»´åº¦è¿›è¡Œå…±äº«ï¼‰ã€‚åœ¨è¿™ä¸ªæ–¹æ³•ä¸­ï¼Œå…ƒå­¦ä¹ å™¨çš„å‚æ•°æ•°é‡å’Œæ¨¡å‹ä¸­çš„å‚æ•°æ•°é‡ä¹‹é—´å¹¶æ²¡æœ‰å‡½æ•°å…³ç³»ã€‚å¦‚æœå…ƒå­¦ä¹ å™¨æ˜¯ä¸€ä¸ªè®°å¿†ç½‘ç»œï¼Œå¦‚ RNNï¼Œæˆ‘ä»¬ä¾ç„¶å¯ä»¥ä»¤æ¨¡å‹ä¸­çš„æ¯ä¸ªå‚æ•°éƒ½å…·æœ‰å•ç‹¬çš„éšè—çŠ¶æ€ï¼Œä»¥ä¿ç•™æ¯ä¸ªå‚æ•°çš„å•ç‹¬å˜åŒ–æƒ…å†µã€‚

**åœ¨ PyTorch ä¸­å®ç°å…ƒå­¦ä¹ **

æˆ‘ä»¬æ¥å°è¯•å†™äº›ä»£ç ï¼Œçœ‹çœ‹çœŸå®æƒ…å†µå¦‚ä½•å§ã€‚

ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªæ¨¡å‹ï¼Œå®ƒåŒ…å«ä¸€ä¸ªæˆ‘ä»¬æƒ³è¦è¿›è¡Œè®­ç»ƒçš„æƒé‡é›†åˆï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¯¥é›†åˆè§£å†³è¿™ä¸¤é¡¹ä»»åŠ¡ï¼š

*   åœ¨å…ƒå‰é¦ˆæ­¥éª¤ä¸­ï¼šæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæ¨¡å‹è®¡ç®—ï¼ˆæŸå¤±å‡½æ•°çš„ï¼‰æ¢¯åº¦ï¼Œå¹¶ä½œä¸ºä¼˜åŒ–å™¨çš„è¾“å…¥æ¥æ›´æ–°æ¨¡å‹å‚æ•°ï¼›

*   åœ¨å…ƒåå‘ä¼ æ’­æ­¥éª¤ä¸­ï¼šæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæ¨¡å‹ä½œä¸ºåå‘ä¼ æ’­ä¼˜åŒ–å™¨å‚æ•°æ¢¯åº¦ï¼ˆä»å…ƒæŸå¤±ä¸­è®¡ç®—å¾—åˆ°ï¼‰çš„è·¯å¾„ã€‚

åœ¨ PyTorch ä¸­å®Œæˆè¿™ä¸ªä»»åŠ¡æœ€ç®€å•çš„æ–¹æ³•æ˜¯ï¼šä½¿ç”¨ä¸¤ä¸ªä¸€æ ·çš„æ¨¡å—æ¥è¡¨ç¤ºæ¨¡å‹ï¼Œæ¯ä¸ªä»»åŠ¡ä¸€ä¸ªã€‚æˆ‘ä»¬æŠŠå­˜å‚¨å…ƒå‰é¦ˆæ­¥éª¤ä¸­ä½¿ç”¨çš„æ¨¡å‹æ¢¯åº¦çš„æ¨¡å—ç§°ä¸ºå‰å‘æ¨¡å‹ï¼ˆforward modelï¼‰ï¼ŒæŠŠå…ƒåå‘ä¼ æ’­æ­¥éª¤ä¸­å°†å‚æ•°å­˜å‚¨ä¸ºåå‘ä¼ æ’­ä¼˜åŒ–å™¨æ¢¯åº¦çš„è¿ç»­è·¯å¾„çš„æ¨¡å—ç§°ä¸ºåå‘æ¨¡å‹ï¼ˆbackward modelï¼‰ã€‚

ä¸¤ä¸ªæ¨¡å—ä¹‹é—´ä¼šä½¿ç”¨å…±äº«çš„ Tensorï¼Œä»¥é˜²æ­¢é‡å¤å ç”¨å†…å­˜ï¼ˆTensor æ˜¯å†…å­˜ä¸­çœŸæ­£æœ‰æ„ä¹‰çš„éƒ¨åˆ†ï¼‰ï¼›ä½†åŒæ—¶ï¼Œä¹Ÿä¼šä¿ç•™å„è‡ªçš„ Variableï¼Œä»¥æ˜ç¡®åŒºåˆ†æ¨¡å‹çš„æ¢¯åº¦å’Œå…ƒå­¦ä¹ å™¨çš„æ¢¯åº¦ã€‚

**PyTorch ä¸­çš„ä¸€ä¸ªç®€å•å…ƒå­¦ä¹ å™¨ç±»**

åœ¨ PyTorch ä¸­å…±äº«å¼ é‡éå¸¸ç›´æ¥ï¼šåªéœ€è¦æ›´æ–° Variable ç±»ä¸­çš„æŒ‡é’ˆï¼Œè®©å®ƒä»¬æŒ‡å‘ç›¸åŒçš„ Tensor å°±å¯ä»¥äº†ã€‚ä½†å¦‚æœæ¨¡å‹å·²ç»æ˜¯å†…å­˜ä¼˜åŒ–æ¨¡å‹ï¼Œä¾‹å¦‚ AWD-LSTM æˆ– AWD-QRNN è¿™ç±»å…±äº« Tensorsï¼ˆè¾“å…¥å’Œè¾“å‡ºåµŒå…¥ï¼‰çš„ç®—æ³•æ—¶ï¼Œæˆ‘ä»¬å°±ä¼šé‡åˆ°é—®éš¾ã€‚è¿™æ—¶ï¼Œæˆ‘ä»¬åœ¨æ›´æ–°ä¸¤ä¸ªæ¨¡å—ä¸­çš„æ¨¡å‹å‚æ•°æ—¶ï¼Œéœ€è¦å¾ˆå°å¿ƒï¼Œä»¥ç¡®ä¿æˆ‘ä»¬ä¿ç•™çš„æŒ‡é’ˆæ˜¯æ­£ç¡®çš„ã€‚

åœ¨è¿™é‡Œç»™å‡ºä¸€ä¸ªå®ç°æ–¹æ³•ï¼šè®¾ç½®ä¸€ä¸ªç®€å•çš„è¾…åŠ©ç¨‹åºæ¥å®Œæˆéå†å‚æ•°çš„ä»»åŠ¡ï¼Œå¹¶è¿”å›æ›´æ–° Parameter æŒ‡é’ˆï¼ˆè€Œä¸åªæ˜¯ Tensorï¼‰æ‰€éœ€çš„å…¨éƒ¨ä¿¡æ¯ï¼Œå¹¶ä¿æŒå…±äº«å‚æ•°åŒæ­¥ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªå®ç°å‡½æ•°ï¼š

```py
def get_params(module, memo=None, pointers=None):
 Â  Â """ Returns an iterator over PyTorch module parameters that allows to update parameters
 Â  Â  Â  Â (and not only the data).
 Â  Â ! Side effect: update shared parameters to point to the first yield instance
 Â  Â  Â  Â (i.e. you can update shared parameters and keep them shared)
 Â  Â Yields:
 Â  Â  Â  Â (Module, string, Parameter): Tuple containing the parameter's module, name and pointer
 Â  Â """
 Â  Â if memo is None:
 Â  Â  Â  Â memo = set()
 Â  Â  Â  Â pointers = {}
 Â  Â for name, p in module._parameters.items():
 Â  Â  Â  Â if p not in memo:
 Â  Â  Â  Â  Â  Â memo.add(p)
 Â  Â  Â  Â  Â  Â pointers[p] = (module, name)
 Â  Â  Â  Â  Â  Â yield module, name, p
 Â  Â  Â  Â elif p is not None:
 Â  Â  Â  Â  Â  Â prev_module, prev_name = pointers[p]
 Â  Â  Â  Â  Â  Â module._parameters[name] = prev_module._parameters[prev_name] # update shared parameter pointer
 Â  Â for child_module in module.children():
 Â  Â  Â  Â for m, n, p in get_params(child_module, memo, pointers):
 Â  Â  Â  Â  Â  Â yield m, n, p 
```

é€šè¿‡è¿™ä¸ªå‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥åµŒå…¥ä»»ä½•æ¨¡å‹ï¼Œå¹¶ä¸”å¾ˆæ•´æ´åœ°éå†å…ƒå­¦ä¹ å™¨çš„æ¨¡å‹å‚æ•°ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬æ¥å†™ä¸€ä¸ªç®€å•çš„å…ƒå­¦ä¹ å™¨ç±»ã€‚æˆ‘ä»¬çš„ä¼˜åŒ–å™¨æ˜¯ä¸€ä¸ªæ¨¡å—ï¼šåœ¨å‰é¦ˆé˜¶æ®µï¼Œå®ƒå¯ä»¥å°†å‰å‘æ¨¡å‹ï¼ˆåŠå…¶æ¢¯åº¦ï¼‰å’Œåå‘æ¨¡å‹ä½œä¸ºè¾“å…¥æ¥å—ï¼Œå¹¶éå†å®ƒä»¬çš„å‚æ•°æ¥æ›´æ–°åå‘æ¨¡å‹ä¸­çš„å‚æ•°ï¼ŒåŒæ—¶å…è®¸å…ƒæ¢¯åº¦åå‘ä¼ æ’­ï¼ˆé€šè¿‡æ›´æ–° Parameter æŒ‡é’ˆï¼Œè€Œä¸ä»…ä»…æ˜¯ Tensor æŒ‡é’ˆï¼‰ã€‚

```py
class MetaLearner(nn.Module):
 Â  Â """ Bare Meta-learner class
 Â  Â  Â  Â Should be added: intialization, hidden states, more control over everything
 Â  Â """
 Â  Â def __init__(self, model):
 Â  Â  Â  Â super(MetaLearner, self).__init__()
 Â  Â  Â  Â self.weights = Parameter(torch.Tensor(1, 2))

 Â  Â def forward(self, forward_model, backward_model):
 Â  Â  Â  Â """ Forward optimizer with a simple linear neural net
 Â  Â  Â  Â Inputs:
 Â  Â  Â  Â  Â  Â forward_model: PyTorch module with parameters gradient populated
 Â  Â  Â  Â  Â  Â backward_model: PyTorch module identical to forward_model (but without gradients)
 Â  Â  Â  Â  Â  Â  Â updated at the Parameter level to keep track of the computation graph for meta-backward pass
 Â  Â  Â  Â """
 Â  Â  Â  Â f_model_iter = get_params(forward_model)
 Â  Â  Â  Â b_model_iter = get_params(backward_model)
 Â  Â  Â  Â for f_param_tuple, b_param_tuple in zip(f_model_iter, b_model_iter): # loop over parameters
 Â  Â  Â  Â  Â  Â # Prepare the inputs, we detach the inputs to avoid computing 2nd derivatives (re-pack in new Variable)
 Â  Â  Â  Â  Â  Â (module_f, name_f, param_f) = f_param_tuple
 Â  Â  Â  Â  Â  Â (module_b, name_b, param_b) = b_param_tuple
 Â  Â  Â  Â  Â  Â inputs = Variable(torch.stack([param_f.grad.data, param_f.data], dim=-1))
 Â  Â  Â  Â  Â  Â # Optimization step: compute new model parameters, here we apply a simple linear function
 Â  Â  Â  Â  Â  Â dW = F.linear(inputs, self.weights).squeeze()
 Â  Â  Â  Â  Â  Â param_b = param_b + dW
 Â  Â  Â  Â  Â  Â # Update backward_model (meta-gradients can flow) and forward_model (no need for meta-gradients).
 Â  Â  Â  Â  Â  Â module_b._parameters[name_b] = param_b
 Â  Â  Â  Â  Â  Â param_f.data = param_b.data 
```

è¿™æ ·ä¸€æ¥ï¼Œæˆ‘ä»¬å°±å¯ä»¥åƒåœ¨ç¬¬ä¸€éƒ¨åˆ†ä¸­çœ‹åˆ°çš„é‚£æ ·æ¥è®­ç»ƒä¼˜åŒ–å™¨äº†ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„è¦ç‚¹ç¤ºä¾‹ï¼Œå±•ç¤ºäº†å‰æ–‡æè¿°çš„å…ƒè®­ç»ƒè¿‡ç¨‹ï¼š

```py
def train(forward_model, backward_model, optimizer, meta_optimizer, train_data, meta_epochs):
 Â """ Train a meta-learner
 Â Inputs:
 Â  Â forward_model, backward_model: Two identical PyTorch modules (can have shared Tensors)
 Â  Â optimizer: a neural net to be used as optimizer (an instance of the MetaLearner class)
 Â  Â meta_optimizer: an optimizer for the optimizer neural net, e.g. ADAM
 Â  Â train_data: an iterator over an epoch of training data
 Â  Â meta_epochs: meta-training steps
 Â To be added: intialization, early stopping, checkpointing, more control over everything
 Â """
 Â for meta_epoch in range(meta_epochs): # Meta-training loop (train the optimizer)
 Â  Â optimizer.zero_grad()
 Â  Â losses = []
 Â  Â for inputs, labels in train_data: Â  # Meta-forward pass (train the model)
 Â  Â  Â forward_model.zero_grad() Â  Â  Â  Â  # Forward pass
 Â  Â  Â inputs = Variable(inputs)
 Â  Â  Â labels = Variable(labels)
 Â  Â  Â output = forward_model(inputs)
 Â  Â  Â loss = loss_func(output, labels) Â # Compute loss
 Â  Â  Â losses.append(loss)
 Â  Â  Â loss.backward() Â  Â  Â  Â  Â  Â  Â  Â  Â  # Backward pass to add gradients to the forward_model
 Â  Â  Â optimizer(forward_model, Â  Â  Â  Â  Â # Optimizer step (update the models)
 Â  Â  Â  Â  Â  Â  Â  Â backward_model)
 Â  Â meta_loss = sum(losses) Â  Â  Â  Â  Â  Â  # Compute a simple meta-loss
 Â  Â meta_loss.backward() Â  Â  Â  Â  Â  Â  Â  Â # Meta-backward pass
 Â  Â meta_optimizer.step() Â  Â  Â  Â  Â  Â  Â  # Meta-optimizer step 
```

**é¿å…å†…å­˜çˆ†ç‚¸â€”â€”éšè—çŠ¶æ€è®°å¿†**

æœ‰æ—¶ï¼Œæˆ‘ä»¬æƒ³è¦å­¦ä¹ ä¸€ä¸ªå¯åœ¨éå¸¸åºå¤§çš„ï¼ˆå¯èƒ½æœ‰å‡ åƒä¸‡ä¸ªå‚æ•°çš„ï¼‰æ¨¡å‹ä¸Šè¿è¡Œçš„ä¼˜åŒ–å™¨ï¼›åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å¸Œæœ›å¯ä»¥åœ¨å¤§é‡æ­¥éª¤ä¸Šå®ç°å…ƒè®­ç»ƒï¼Œä»¥å¾—åˆ°ä¼˜è´¨æ¢¯åº¦ï¼›å°±åƒæˆ‘ä»¬åœ¨è®ºæ–‡ã€ŠMeta-Learning a Dynamical Language Modelã€‹ä¸­æ‰€å®ç°çš„é‚£æ ·ã€‚

åœ¨å®è·µä¸­ï¼Œè¿™æ„å‘³ç€ï¼Œæˆ‘ä»¬æƒ³è¦åœ¨å…ƒå‰é¦ˆä¸­åŒ…å«ä¸€ä¸ªå¾ˆé•¿çš„è®­ç»ƒè¿‡ç¨‹ï¼Œä»¥åŠå¾ˆå¤šæ—¶é—´æ­¥ï¼›åŒæ—¶æˆ‘ä»¬è¿˜éœ€è¦å°†æ¯ä¸€æ­¥çš„å‚æ•°ï¼ˆé»„è‰²â– ï¼‰å’Œæ¢¯åº¦ï¼ˆç»¿è‰²â– ï¼‰ä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œè¿™äº›å‚æ•°å’Œæ¢¯åº¦ä¼šåœ¨å…ƒåå‘ä¼ æ’­ä¸­ä½¿ç”¨åˆ°ã€‚

æˆ‘ä»¬å¦‚ä½•åœ¨ä¸è®© GPU å†…å­˜çˆ†ç‚¸çš„æƒ…å†µä¸‹åšåˆ°è¿™ä¸€ç‚¹å‘¢ï¼Ÿ

ä¸€ä¸ªåŠæ³•æ˜¯ï¼Œä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆgradient checkpointingï¼‰æ¥ç”¨å†…å­˜æ¢å–è®¡ç®—ï¼Œè¿™ä¸ªæ–¹æ³•ä¹Ÿå«ã€Œéšè—çŠ¶æ€è®°å¿†ã€ï¼ˆHidden State Memorizationï¼‰ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæ¢¯åº¦æ£€æŸ¥ç‚¹è¡¨ç¤ºï¼Œå°†æˆ‘ä»¬è¿ç»­è®¡ç®—çš„å…ƒå‰é¦ˆå’Œå…ƒåå‘ä¼ æ’­åˆ‡åˆ†æˆç‰‡æ®µã€‚

æ¥è‡ª Open AI çš„ Yaroslav Bulatov æœ‰ä¸€ç¯‡å¾ˆå¥½çš„ä»‹ç»æ¢¯åº¦æ£€æŸ¥ç‚¹çš„æ–‡ç« ï¼Œå¦‚æœä½ æ„Ÿå…´è¶£ï¼Œå¯ä»¥äº†è§£ä¸€ä¸‹ï¼š

Fitting larger networks into memoryï¼ˆhttps://medium.com/@yaroslavvb/fitting-larger-networks-into-memory-583e3c758ff9ï¼‰

è¿™ç¯‡æ–‡ç« éå¸¸é•¿ï¼Œæ‰€ä»¥æˆ‘æ²¡æœ‰ç»™å‡ºä¸€ä¸ªå®Œæ•´çš„æ¢¯åº¦æ£€æŸ¥ç‚¹ä»£ç ç¤ºä¾‹ï¼Œå»ºè®®å¤§å®¶ä½¿ç”¨å·²ç»å¾ˆå®Œå–„çš„ TSHadley çš„ PyTorch å®ç°ï¼Œä»¥åŠå½“å‰è¿˜åœ¨å¼€å‘çš„æ¢¯åº¦æ£€æŸ¥ç‚¹çš„ PyTorch æœ¬åœ°å®ç°ã€‚

**å…ƒå­¦ä¹ ä¸­çš„å…¶ä»–æ–¹æ³•**

å…ƒå­¦ä¹ ä¸­è¿˜æœ‰å¦å¤–ä¸¤ä¸ªå¾ˆæœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ï¼Œä½†æœ¬æ–‡æ²¡æœ‰æ—¶é—´æ¥è®¨è®ºäº†ã€‚åœ¨è¿™é‡Œæˆ‘ç»™å‡ºä¸€äº›æç¤ºï¼Œè¿™æ ·ï¼Œå½“ä½ çŸ¥é“äº†å®ƒä»¬å¤§è‡´çš„åŸç†åï¼Œå°±å¯ä»¥è‡ªå·±æŸ¥é˜…ç›¸å…³èµ„æ–™äº†ï¼š

*   å¾ªç¯ç¥ç»ç½‘ç»œï¼šæˆ‘ä»¬ä¹‹å‰ç»™å‡ºäº†ç¥ç»ç½‘ç»œçš„æ ‡å‡†è®­ç»ƒè¿‡ç¨‹ã€‚è¿˜æœ‰ä¸€ä¸ªæ–¹æ³•ï¼šå°†è¿ç»­çš„ä»»åŠ¡ä½œä¸ºä¸€ä¸ªè¾“å…¥åºåˆ—ï¼Œç„¶åå»ºç«‹ä¸€ä¸ªå¾ªç¯æ¨¡å‹ï¼Œå¹¶ç”¨å®ƒæå–ã€æ„å»ºä¸€ä¸ªå¯ç”¨äºæ–°ä»»åŠ¡çš„åºåˆ—è¡¨å¾ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œå¯¹äºæŸä¸ªå¸¦æœ‰è®°å¿†æˆ–æ³¨æ„åŠ›çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬é€šå¸¸åªä½¿ç”¨ä¸€ä¸ªè®­ç»ƒè¿‡ç¨‹ã€‚è¿™ä¸ªæ–¹æ³•çš„æ•ˆæœä¹Ÿå¾ˆä¸é”™ï¼Œå°¤å…¶æ˜¯å½“ä½ è®¾è®¡å‡ºé€‚åˆä»»åŠ¡çš„åµŒå…¥æ—¶ã€‚æœ€è¿‘çš„è¿™ç¯‡ SNAIL è®ºæ–‡æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ï¼šA Simple Neural Attentive Meta-Learnerï¼ˆhttps://openreview.net/forum?id=B1DmUzWAWï¼‰ã€‚

*   å¼ºåŒ–å­¦ä¹ ï¼šä¼˜åŒ–å™¨åœ¨å…ƒå‰é¦ˆè¿‡ç¨‹ä¸­å®Œæˆçš„è®¡ç®—å’Œå¾ªç¯ç¥ç»ç½‘ç»œçš„è®¡ç®—è¿‡ç¨‹å¾ˆç±»ä¼¼ï¼šåœ¨è¾“å…¥åºåˆ—ï¼ˆå­¦ä¹ è¿‡ç¨‹ä¸­æ¨¡å‹çš„æƒé‡åºåˆ—å’Œæ¢¯åº¦åºåˆ—ï¼‰ä¸Šé‡å¤ä½¿ç”¨ç›¸åŒçš„å‚æ•°ã€‚åœ¨çœŸå®åœºæ™¯ä¸‹ï¼Œè¿™è¡¨ç¤ºæˆ‘ä»¬ä¼šé‡åˆ°å¾ªç¯ç¥ç»ç½‘ç»œç»å¸¸é‡åˆ°çš„ä¸€ä¸ªé—®é¢˜ï¼šä¸€æ—¦æ¨¡å‹å‡ºé”™ï¼Œå°±å¾ˆéš¾è¿”å›å®‰å…¨è·¯å¾„ï¼Œå› ä¸ºæˆ‘ä»¬å¹¶æ²¡æœ‰è®­ç»ƒæ¨¡å‹ä»è®­ç»ƒè¯¯å·®ä¸­æ¢å¤çš„èƒ½åŠ›ï¼›åŒæ—¶ï¼Œå½“é‡åˆ°ä¸€ä¸ªæ¯”å…ƒå­¦ä¹ è¿‡ç¨‹ä¸­ä½¿ç”¨çš„åºåˆ—æ›´é•¿çš„åºåˆ—æ—¶ï¼Œæ¨¡å‹éš¾ä»¥æ³›åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥æ±‚åŠ©äºå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œè®©æ¨¡å‹å­¦ä¹ ä¸€ä¸ªå’Œå½“å‰è®­ç»ƒçŠ¶æ€ç›¸å…³çš„åŠ¨ä½œç­–ç•¥ã€‚

**è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å…ƒå­¦ä¹ **

å…ƒå­¦ä¹ å’Œç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆå¦‚å¾ªç¯ç¥ç»ç½‘ç»œï¼‰ä¹‹é—´æœ‰ä¸€ä¸ªéå¸¸æœ‰è¶£çš„ç›¸ä¼¼ä¹‹å¤„ã€‚åœ¨ä¸Šä¸€æ®µä¸­ï¼Œæˆ‘ä»¬æ›¾æåˆ°ï¼š

> ç”¨äºä¼˜åŒ–ç¥ç»ç½‘ç»œæ¨¡å‹çš„å…ƒå­¦ä¹ å™¨çš„è¡Œä¸ºå’Œå¾ªç¯ç¥ç»ç½‘ç»œç±»ä¼¼ã€‚

å’Œ RNN ç±»ä¼¼ï¼Œå…ƒå­¦ä¹ å™¨ä¼šæå–ä¸€ç³»åˆ—æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‚æ•°å’Œæ¢¯åº¦ä½œä¸ºè¾“å…¥åºåˆ—ï¼Œå¹¶æ ¹æ®è¿™ä¸ªè¾“å…¥åºåˆ—è®¡ç®—å¾—åˆ°ä¸€ä¸ªè¾“å‡ºåºåˆ—ï¼ˆæ›´æ–°åçš„æ¨¡å‹å‚æ•°åºåˆ—ï¼‰ã€‚

æˆ‘ä»¬çš„è®ºæ–‡ã€ŠMeta-Learning a Dynamical Language Modelã€‹ä¸­è¯¦ç»†è®ºè¿°äº†è¯¥ç›¸ä¼¼æ€§ï¼Œå¹¶ç ”ç©¶äº†å°†å…ƒå­¦ä¹ å™¨ç”¨äºç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥å®ç°ä¸­æœŸè®°å¿†ï¼šç»è¿‡å­¦ä¹ ï¼Œå…ƒå­¦ä¹ å™¨èƒ½å¤Ÿåœ¨æ ‡å‡† RNNï¼ˆå¦‚ LSTMï¼‰çš„æƒé‡ä¸­ï¼Œç¼–ç ä¸­æœŸè®°å¿†ï¼ˆé™¤äº†çŸ­æœŸè®°å¿†åœ¨ LSTM éšè—çŠ¶æ€ä¸­çš„ä¼ ç»Ÿç¼–ç æ–¹å¼ä»¥å¤–ï¼‰ã€‚

![](img/f33cbcef1527008d9fc50c97ef0f6575-fs8.png)

æˆ‘ä»¬çš„å…ƒå­¦ä¹ è¯­è¨€æ¨¡å‹ç”± 3 å±‚è®°å¿†å±‚çº§ç»„æˆï¼Œè‡ªä¸‹è€Œä¸Šåˆ†åˆ«æ˜¯ï¼šæ ‡å‡† LSTMã€ç”¨äºæ›´æ–° LSTM æƒé‡ä»¥å­˜å‚¨ä¸­æœŸè®°å¿†çš„å…ƒå­¦ä¹ å™¨ï¼Œä»¥åŠä¸€ä¸ªé•¿æœŸé™æ€è®°å¿†ã€‚

æˆ‘ä»¬å‘ç°ï¼Œå…ƒå­¦ä¹ è¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡è®­ç»ƒæ¥ç¼–ç æœ€è¿‘è¾“å…¥çš„è®°å¿†ï¼Œå°±åƒä¸€ç¯‡ç»´åŸºç™¾ç§‘æ–‡ç« çš„å¼€å§‹éƒ¨åˆ†å¯¹é¢„æµ‹æ–‡ç« çš„ç»“å°¾éƒ¨åˆ†éå¸¸æœ‰å¸®åŠ©ä¸€æ ·ã€‚

![](img/989fd10fbb7a12d4b79144831d3138d8-fs8.png)

*ä¸Šå›¾ä¸­çš„æ›²çº¿å±•ç¤ºäº†åœ¨ç»™å®šä¸€ç¯‡ç»´åŸºç™¾ç§‘æ–‡ç« å¼€å§‹éƒ¨åˆ†çš„æƒ…å†µä¸‹ï¼ˆA, â€¦, H æ˜¯è¿ç»­çš„ç»´åŸºç™¾ç§‘æ–‡ç« ï¼‰ï¼Œæ¨¡å‹é¢„æµ‹æ–‡ç« è¯æ±‡çš„æ•ˆæœã€‚å•è¯é¢œè‰²è¡¨ç¤ºçš„æ„æ€ç›¸åŒï¼šè“è‰²è¡¨ç¤ºæ›´å¥½ï¼Œçº¢è‰²è¡¨ç¤ºæ›´å·®ã€‚å½“æ¨¡å‹åœ¨é˜…è¯»ä¸€ç¯‡æ–‡ç« æ—¶ï¼Œå®ƒä»æ–‡ç« çš„å¼€å§‹éƒ¨åˆ†è¿›è¡Œå­¦ä¹ ï¼Œè¯»åˆ°ç»“å°¾éƒ¨åˆ†çš„æ—¶å€™ï¼Œå®ƒçš„é¢„æµ‹æ•ˆæœä¹Ÿå˜å¾—æ›´å¥½äº†ï¼ˆæ›´å¤šç»†èŠ‚ï¼Œè¯·é˜…è¯»æˆ‘ä»¬çš„è®ºæ–‡ï¼‰ã€‚*

ä»¥ä¸Šæ˜¯æˆ‘å¯¹å…ƒå­¦ä¹ çš„ä»‹ç»ï¼Œå¸Œæœ›å¯¹å¤§å®¶æœ‰æ‰€å¸®åŠ©ï¼![](img/2d1c94eb4a4ba15f356c96c72092e02b-fs8.png)

**å‚è€ƒæ–‡çŒ®**

*1\. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#afeb) As such, meta-learning can be seen as a generalization ofã€Œtransfer learningã€and is related to the techniques for fine-tuning model on a task as well as techniques for hyper-parameters optimization. There was an interesting workshop on meta-learning (https://nips.cc/Conferences/2017/Schedule?showEvent=8767) at NIPS 2017 last December.*

*2\. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#dc5a) Of course in a real training we would be using a mini-batch of examples.*

*3\. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#e0bb) More precisely:ã€Œmost ofã€these operations are differentiable.*

*4\. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#d640) Good blog posts introducing the relevant literature are the BAIR posts: Learning to learn (http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/) by Chelsea Finn and Learning to Optimize with Reinforcement Learning (http://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/) by Ke Li.*

*5\. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#930c) Good examples of learning the model initial parameters are Model-Agnostic Meta-Learning (https://arxiv.org/abs/1703.03400) of UC Berkeley and its recent developments (https://openreview.net/forum?id=BJ_UL-k0b) as well as the Reptile algorithm (https://blog.openai.com/reptile/) of OpenAI. A good example of learning the optimizerã€s parameters is the Learning to learn by gradient descent by gradient descent (https://arxiv.org/abs/1606.04474) paper of DeepMind. A paper combining the two is the work Optimization as a Model for Few-Shot Learning (https://openreview.net/forum?id=rJY0-Kcll) by Sachin Ravi and Hugo Larochelle. An nice and very recent overview can be found in Learning Unsupervised Learning Rules (https://arxiv.org/abs/1804.00222).*

*6\. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#d094) Similarly to the way we back propagate through time in an unrolled recurrent network.*

*7\. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#725d) Initially described in DeepMindã€s Learning to learn by gradient descent by gradient descent (https://arxiv.org/abs/1606.04474) paper.*

*8\. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#4e23) We are using coordinate-sharing in our meta-learner as mentioned earlier. In practice, it means we simply iterate over the model parameters and apply our optimizer broadcasted on each parameters (no need to flatten and gather parameters like in L-BFGS for instance).*

*9\. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#d029) There is a surprising under-statement of how important back-propagating over very long sequence can be to get good results. The recent paper An Analysis of Neural Language Modeling at Multiple Scales (https://arxiv.org/abs/1803.08240) from Salesforce research is a good pointer in that direction.*

*10\. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#6c6f) Gradient checkpointing is described for example in Memory-Efficient Backpropagation Through Time (https://arxiv.org/abs/1606.03401) and the nice blog post (https://medium.com/@yaroslavvb/fitting-larger-networks-into-memory-583e3c758ff9) of Yaroslav Bulatov.*

*åŸæ–‡é“¾æ¥ï¼šhttps://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a*

 ******æœ¬æ–‡ä¸ºæœºå™¨ä¹‹å¿ƒç¼–è¯‘ï¼Œ**è½¬è½½è¯·è”ç³»æœ¬å…¬ä¼—å·è·å¾—æˆæƒ****ã€‚**

âœ„------------------------------------------------

**åŠ å…¥æœºå™¨ä¹‹å¿ƒï¼ˆå…¨èŒè®°è€…/å®ä¹ ç”Ÿï¼‰ï¼šhr@jiqizhixin.com**

**æŠ•ç¨¿æˆ–å¯»æ±‚æŠ¥é“ï¼šeditor@jiqizhixin.com**

**å¹¿å‘Š&å•†åŠ¡åˆä½œï¼šbd@jiqizhixin.com****