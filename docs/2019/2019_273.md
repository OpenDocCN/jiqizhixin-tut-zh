# 7年斩获15金，最高全球第8：Kaggle Grandmaster分享竞赛经验

> 原文：[http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650775876&idx=3&sn=c2a74c3238fac67d2c3f83b13a219af1&chksm=871a653ab06dec2cd440a0090d30a39b9905d488a455a0c1d4de94e15b86a698b260761604d9&scene=21#wechat_redirect](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650775876&idx=3&sn=c2a74c3238fac67d2c3f83b13a219af1&chksm=871a653ab06dec2cd440a0090d30a39b9905d488a455a0c1d4de94e15b86a698b260761604d9&scene=21#wechat_redirect)

机器之心专栏

**机器之心经授权转载**

> 这是 Kaggle 你问我答 (AMA) 的第二期活动，本期请到的嘉宾是 Jiwei Liu，他博士毕业于匹兹堡大学，目前是英伟达的一名高级数据科学家。

*   Kaggle profile：https://www.kaggle.com/jiweiliu

他 7 年前开始参加 kaggle 竞赛，Tabular Data/Computer Vision 比赛均有涉猎。截至目前共斩获 15 金 28 银 11 铜，kaggle 最高排名全球第 8。

![](../Images/9311a612c5f257f362b75da5870c82c2.jpg)

以下是本期活动的问答集锦：**Q1: 比赛初始阶段，需要做一个长期的计划吗，比赛期间时间如何分配？**A1: 我其实是很没计划的一个人，对于比赛我基本上是选对自己最有利的参加，一般看中的条件有：1\. 数据量特别大；2\. CV (cross-validation) 比较稳定；3\. sota 算法我比较熟悉。前期疯狂怼时间，早日冲入第一梯队，然后就是慢慢磨，看运气了。**Q2: 怎么 develop 自己的比赛 pipeline？有什么大神的例子可以参考？**A2: 这个问题很好。我一直觉得要有一个自己的 pipeline，然后慢慢的把其他人的代码加入到自己的 pipeline 里面。通过这个改写的过程能发现一些细节问题，也能理解的更深入一点。我也在找一个代码风格比较好的模版来开发自己的 pipeline，希望大家也推荐一下。我目前觉得 fastai 的不错，想把自己之前的用 fastai 重写。我的一个想法就是，把自己的 pipeline 当成一个开源项目来做，时机成熟的时候分享出去，让其他人可以 pip 安装，这是我的一个目标。**Q3: 很多参赛者只是抄一抄 kernel, ensemble 一下，但分会很高，比赛时该如何调整心态，建立比其他参赛者的优势？**A3: 心态是最难的。我觉得做 kaggle 的一个小目标就是压住 kernel，要比最好的 kernel 要好。我相信大家也有类似的经验，超过 best public kernel 以后排名也就基本稳住了。自己阅读 best single model kernel 是很有用的，尤其是一些新鲜的操作。但也有很多时候，我发现 kernel 会过度堆特征，很多 overkill。如果想清楚其中的关键，简化 best public kernel 是一个有效的办法。对于表格赛，我比较在意要清楚每一块特征的重要性，可有可无的特征要坚决的去掉。我会去 incremental 的一点点加特征，验证有效性。**Q4: 请问一个新人应该如何寻找队友？**A4: 好问题，我回忆一下。一个是要和 kaggler 们建立良好的互动，比如积极参与论坛里面的讨论，尤其是针对算法或者模型细节的。如果有什么新的发现也及时的分享。我推荐大家和国外的 kaggler 们组队，要勇敢的套磁，向 giba 学习。**Follow-up: 为什么建议和国外的 kaggler 组队呢？**Answer: 主要是结识一些大佬。我当时找实习的时候，一个队友给我推荐了很多机会，甚至还能找到投资人。**Q5: 请问从 Kaggle 比赛里所产生的模型离直接运用到真实应用环境里有多远？**A5: 真实环境其实也是也有很多种不同情况，不能一概而论。Kaggle 的模型特别适合做 demo，或者 benchmark。比如 criteo 在 kaggle 的 ctr 比赛后来扩展成了 criteo 1tb benchmark，这是现在做 big data ctr 一个很常用的 benchmark。google，fb 还有我们都在用这个 benchmark 做 demo，里面你能看到很多当时比赛中的模型和技巧。还有就是，kaggle 模型可以作为一个很好的 baseline。有一些实际情况，特别在意准确率和速度的 tradeoff，在没有好的 baseline 我都会推荐去 kaggle 上找找类似的比赛。其实我的工作和 ml in production 还有一定距离。我和一些业界朋友聊天的结果是，其实 kaggle trick 在工业中应用还挺多的，比我想象的要多。我们英伟达还有一个主要的考量是 scalability，一个模型可以做成 multi-node，multi-gpu 是我们的一个基本要求。**Q6: 如何高效的 feature engineering？**A6: 这个可以分成两方面吧，一个是从 trick 出发，一个是从 domain knowledge 出发。首先常用的 trick，像是 mean target encoding，或者 timestamp difference，这些都可以先无脑试一下。但是最终像 chris，cpmp 这些大神都会仔细分析一下为什么这个 feature 有用，理解了之后就能做出更强的 feature。domain knowledge 比较考验检索信息的能力，不只是 paper，任何关于数据集的背景资料都可以作为 feature engineering 的灵感。建议大家详细的读 kaggle 的 data 介绍还有比赛介绍，管理员的一言一行也要仔细观察，我见过太多次，从这里面发现 leak 苗头的。**Q7: 在 eda 的时候一般会怎么分析呢？重点关注哪些指标？**A7: 我最关心的是特征的分布在 train 和 test 里面是否一致，我一般会写一个 routine，检查 numerical feature 和 categorical feature 在 train 和 test 里面的差异情况。我会把很不一致，或者不是很一致的 feature 都先 highlight 一下。如果分布一致的话，先用原始的 feature 跑一个 lgb/xgb cv，观察一下 cv 和 lb 的一致性，是否受某几个 feature 影响剧烈，相当于再次印证一下之前的一致性分析。然后就是考察每一个 feature 的重要性和彼此之间的相关性。大家也看到了，很多表格赛中都有隐藏的 group 信息，就是某些 sample 其实是一个 group，这也是 eda 时候要检查的。**Q8: 造特征的时候怎么找方向感？**A8: 这个可以观察 lb 的分布来找灵感。如果 lb 是那种有大 gap 的，那极有可能是有 leak 或者强特，而强特又经常是简单的 single feature。这种找 leak 可能是一个比较漫长痛苦的过程，通常是要理解数据背后的故事。每个比赛都不太相同了，大家可以回顾有 leak 的比赛。看看 leak 当时是怎么被发现的。如果 lb 分布很匀，那堆特征的可能性比较大了，基本的 1way，2way 的 target encoding 和 count encoding 都是可以无脑尝试的。还有一个注意点就是关注 lb 是怎么分的，如果 public 和 private 是随机分，一定不要忘记 prob 的可能性。当年有一个传奇的蚊子比赛，第一名提前三个月制定好了 prob 计划，用 100 多个 submission prob，raddar 在 2016 年 red hat 比赛的冠军方案中也有一个经典的prob解法。**Q9: 一般如何检验一批特征是否有效？会画图 (feature vs target) 检验么？**A9: 我比较不擅长画图，从 chris 的分享看，画图是非常有用的。我一般还是直接看 CV 的分数来选择了。**Q10: 请问 Tabular 比赛有自己独创的 pipeline 吗，大概是什么样的呢？**A10: 还是可以有的，一个是收集经典的各种 encoding 代码。我有一段时间会尝试着写成 sklearn transformer 的 api，但后来发现也不是必要的，尤其有时候要 train 和 test data 一起 fit。我觉得大部分 feature engineering 的操作都能从 kernel 里面找到，至少用到的时候能想起来去哪个 kernel 找。**Q11: 感觉相比 CV(Compute Vision) 的比赛，Tabular 比赛很难找排名上升的切入点，经常停滞在某个精度，请问这个时候有什么比较好的步骤或者窍门吗？**A11: 太真实了。我最近和很多 kaggler 聊，他们都在把自己的工作重心转到 CV 了，一个最重要的理由就是 CV 的 pipleline 熟了以后真的很省时间，基本迁移到新比赛上做的改动比表格赛小的多，与此对应的就是表格赛真的不太有这种万灵药，做表格赛首先要抱着探索的心态。chris，cpmp，raddar，giba 和 the zoo 可以说是业界标杆，大家要 follow 他们在表哥赛中的分享和讨论，如果他们在前排，至少证明比赛是 valid，是可以依靠技巧的。这个技巧可能更多的是数据分析，我印象里 raddar 和 giba，还有 dmitry larko，非常会用 excel，经常 sample 出一个小表格，在 excel 里一通骚操作，然后肉眼找 pattern。**Q12: 能够这么长时间的投入 kaggle 拿这么多奖牌真的觉得很厉害，请问做 kaggle 的动力是什么，怎么规划时间？**A12: 动力有很多，有一段时间是生活所迫，特别缺钱。找一个长期的队友也很有帮助。当然了，建立正反馈永远是最有效的，如果 kaggle 的好成绩对你的学习工作很重要，那我相信动力会越来越强的。但说实在话，现在是我动力最弱的一段时间，一个是家庭原因，还有就是想丰富一下自己的技术栈。最近做比赛的时间非常少了，不过我觉得还是会回归的。**Q13: 请问做好表格赛需要哪些技术栈和重要的 trick?**A13: 是时候推荐一下 rapids.ai 了。虽然从功能上，rapids ai 和 pandas，sklearn，scipy，networkx 没有太大区别，但速度快了以后，可以允许做 exhaustive search。现在 rapids 的 speedup 基本上在 10x～100x，有很多操作都能到 100x speedup。pandas 大家就比较熟了，我们对应的是 cudf，我就不多讲。比较推荐大家关注一下，cusignal 和 cugraph，如果是信号或者 graph data 相关的表格赛，用这两个库无脑尝试各种特征是非常爽的。**Q14: 对于没有接触过的题目，除了 kaggle 的讨论区和 kernel，还会从哪些地方来学习呢，是相关任务 sota 的论文吗?**A14: 我还是会先找 kaggle 上有过的类似比赛，当然这个讨论区里应该会提到的。论文当然是很好的，尤其是有代码的论文。我也会搜索一下博客，比如像 medium 这种。还有就是 pytorch 和 tensorflow 里面的 demo，更多还是日常积累吧。**Q15: 请问作为一个机器学习工程师，应该需要有多少其他计算机科学领域的知识，（计算机系统，编程语言等）？作为在校生应该如何准备适应将来业界中的工作？**A15: 对于本科生我建议了解的越宽泛越好，我个人理解 ml 处于 hype 中后期的平稳阶段，需要和传统的计算机技术深度融合，所以我建议多了解。工作是很漫长的，先工作起来，自然就清楚啦。**Q16: 当年和陈天奇同场竞技的感受如何？**A16: 谈不上竞技，就是向天奇大佬学习。看到 xgb 一点点做起来还是感受挺深的。现在也称得上是业界标杆，真的棒。**Q17: 现在的比赛状态，还有必要复习 16 年及以前的 winner solution 吗？**A17: 我觉得要看具体案例，比如 sjv 的代码，我觉得现在看也不过时。**Q18: 打 kaggle 这么多年，觉得 kaggle 这些年有什么变化吗？对 kaggle 的未来有什么展望？**A18: 最大的变化是 cv 比赛多了，但奖金已经没有两三年前爆炸了。kaggle 正在变成 google cloud 的试验场。当然我听说 kaggle 的生意还是不错的，不缺赞助商，也不缺比赛，但是像总奖金 25000 这种的比赛会越来越多。Kaggle 的需求是充足的，他们没有盲目增加比赛数量，而是保证质量和多样性。越来越多的 google 产品，像 tpu，automl 都会亮相 kaggle。kaggle 和 google colab 可以说是 google 通过开源和免费继续拓展用户。kaggle 的竞技性可能会减弱，教学性还有辅助研究的作用可能会加强。

*原文链接：**https://zhuanlan.zhihu.com/p/94897334*

**[机器之心「SOTA模型」](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650770891&idx=1&sn=25bde35991047a997337c8dd25350089&chksm=871a49b5b06dc0a36fc3407e3643550ef97f72b007e67c4f4be250bfd60c9fdc5389624569c0&scene=21#wechat_redirect)****：****22****大领域、127个任务，机器学习 SOTA 研究一网打尽。****[![](../Images/b9b6a80298070cc7bfd0977f3781a267.jpg)](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650770891&idx=1&sn=25bde35991047a997337c8dd25350089&chksm=871a49b5b06dc0a36fc3407e3643550ef97f72b007e67c4f4be250bfd60c9fdc5389624569c0&scene=21#wechat_redirect)**

点击阅读原文，立即访问。