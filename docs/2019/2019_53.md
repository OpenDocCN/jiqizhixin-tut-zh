# 手把手教你将矩阵画成张量网络图

> 原文：[http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650762696&idx=2&sn=9ebbc88f0c855af0b7e22046a473eab4&chksm=871aa9b6b06d20a0e3b87930d9e608885f6532220d656ef63f30b8968f20daa821b9f5cc5048&scene=21#wechat_redirect](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650762696&idx=2&sn=9ebbc88f0c855af0b7e22046a473eab4&chksm=871aa9b6b06d20a0e3b87930d9e608885f6532220d656ef63f30b8968f20daa821b9f5cc5048&scene=21#wechat_redirect)

选自math3ma

**作者：Algebra**

**机器之心编译**

**参与：李志伟、张倩**

> 在之前的一篇文章中，我们介绍过如何将[矩阵&概率画成图](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650759406&idx=3&sn=aba9ba0745faaadfcb33efd948cce36f&chksm=871aa490b06d2d8647bd4b93afd1ee809633fe09d68884f4364035420bc3ab25b235ea323f9b&scene=21#wechat_redirect)，读者表示妙不可言。最近，该文作者又动手实现了新的想法：将矩阵画成张量网络图。

今天，我想分享一种不同的方法来描绘矩阵，它不仅用于数学，也用于物理、化学和机器学习。基本想法是：一个带有实数项的 m×n 矩阵 M 可以表示从 R^n→R^m 的线性映射。这样的映射可以被描绘成具有两条边的节点。一条边表示输入空间，另一条边表示输出空间。

![](../Images/ac066083d2ad8aea69417531428fa970.jpg)

我们可以用这个简单的想法做很多事情。但首先，要指定 m×n 的矩阵 M，必须指定所有 mn 项 M_ij。索引 i 的范围从 1 到 m，表示输出空间的维数；j 的范围从 1 到 n，表示输入空间的维数。换言之，i 表示 M 的行数，j 表示其列数。如果我们愿意，这些符号可以包括在图中：

![](../Images/8e1aa576cdbf2cd2f8cae7d5cebaf35f.jpg)

这个想法很容易概括。矩阵是一个二维的数组，而一个 n 维的数组被称为一个 n 阶张量或一个 n-张量。像矩阵一样，一个 n 张量可以用一个节点来表示，每个维度有一个边。

例如，一个数字可以被认为是一个零维数组，即一个点。因此，它是一个 0-张量，可以绘制为一个边为零的节点。同样地，一个向量可以被认为是一个一维的数组，因此是一个 1-张量。它由一个具有一条边的节点表示。矩阵是二维数组，因此是 2-张量。它由一个有两条边的节点表示。三维张量是一个三维数组，因此是一个有三条边的节点……。

![](../Images/29722ecd8262f7e73003213bb9e1e250.jpg)

**矩阵乘法是张量的缩并**

将两个矩阵相乘就相当于「粘合」它们的图。这叫做张量的缩并（tensor contraction）。

![](../Images/21850362ed230202ae5eb41f218228ca.jpg)

在上图中，具有相同索引 j 的边是缩并的边。这与两个矩阵只有在输入/输出维度匹配时才能相乘的事实是一致的。你还会注意到结果图片有两个自由索引，即 i 和 k，它们确实定义了一个矩阵。

顺便说一下，画出这些图的一个关键特征是我们不必携带索引。

速查：矩阵被描述为一个单节点，每个向量空间有一个边，但是上面的图片有两个节点。我们仍然希望它表示一个矩阵。我可以断言，它还是一个矩阵！有一个很好的方法可以让我们看出来：将蓝色和绿色节点碰在一起。

![](../Images/bfd5b196a692e8c26443037c88df78f4.jpg)

这让我想起雨水从窗户滴下来：当两个雨滴接触时，它们融合成更大的雨滴。这是矩阵乘法。对于矩阵向量乘法，也有类似的情况：一个矩阵 M 乘以一个向量 v，得到另一个向量 Mv，它是一个具有一个自由边的节点。

![](../Images/7604e9b5e67b82d2f3572cee3306e524.jpg)

更通俗地说，两个或更多张量的乘积由一组节点和边表示，其中具有相同索引的边发生缩并。

![](../Images/0e0080b987be80a6955c36c37cb7def3.jpg)

**节点形状可以表示不同的属性**

以上的节点都是用圆表示的，但这只是其中一种选择。没有人规定必须使用哪种形状。这意味着我们可以发挥创造力！例如，我们可能只想为对称矩阵保留一个圆形或其他对称形状，如正方形。

![](../Images/8b35a76e55088b6b171d1aacb60376e5.jpg)

然后矩阵的转置可以通过反转其图像来表示：

![](../Images/c6ca724c8fbcfbb5ed575f3348b9ce78.jpg)

所以对称矩阵的对称性保留在图中！

![](../Images/e0626476068ddae2ce63643d71b6f3bd.jpg)

我也喜欢将等距嵌入（isometric embedding）绘制为三角形的想法：

![](../Images/0fc4b1b7c62fc177379078d647a25799.jpg)

等距嵌入 U 是从空间 V 到更大维度空间 W 的线性映射，它保留了向量的长度。这样的图满足 U^⊤U=id_v，但 UU^⊤≠id_w。换句话说，你可以将小空间 V 嵌入到大空间，然后再投影回 V 中，而不扭曲 V 中的向量（与拓扑中的回缩映射（retraction map）不同）。但是将所有的 W 都压缩到小 V 上后，你不能指望在将 V 转回 W 的过程中修复损坏。三角形暗示了这种大与小的特征。（三角形的底边比它的尖端大！）一般来说，如下图所示，单位线性算子被画成直线：

![](../Images/d050fbd00400731996303d57cef2519e.jpg)

**矩阵分解也可以画出很好的图**

在讨论矩阵乘法，即矩阵合成的时候，我们不要忘记矩阵分解！例如，每个矩阵都有一个奇异值分解。这是一张与之相关的很棒的图片：

![](../Images/f0fd0199331cb1626127db0a56f89798.jpg)

这里，U 和 V 是一元矩阵，所以是等距矩阵，也是三角形。矩阵 D 是一个对角矩阵，我喜欢用一个菱形来表示。总之，矩阵分解是将一个节点分解为多个节点；矩阵乘法是将多个节点融合为一个节点。

![](../Images/60670ae6b1c0a3e766debf2012bd9b08.jpg)

上图说明了这些图的另一个特点：节点的空间位置并不重要。我可以画黄色、蓝色、绿色和粉色的节点，在水平线、垂直线或之字形等任何我想画的形状上。唯一重要的是图有两个自由边。矩阵的乘积是另一个矩阵！

**混乱的证明简化为图的证明。**

关于这个图形符号，我们还有更多想说的，但我将用另一个值得注意的特性来总结：证明过程可以变得非常简单！以矩阵的迹为例。矩阵的迹图很简单。它被定义为一个共同索引的总和：

![](../Images/d67f1517ef1810f055212477f086e424.jpg)

这串图没有自由边。这是一个循环。这与迹是一个数字的事实是一致的，它是一个 0 张量，所以它没有自由索引。这里有一个证明，在循环排列下，迹是不变的：

![](../Images/c68ab47a054d4a6618ec8ba608bc2ad0.jpg)

把珠子沿着项链滑。好简洁！

**命名之争**

文章中讨论的图起源于 Penrose 的图形符号，被称为张量网络图和/或字符串图（string diagram），也许有一些微小的区别。「和/或」取决于你是谁。也就是说，在物理/机器学习社区（在那里它们被称为张量网络图）和范畴论社区（在那里它们被称为字符串图），将向量空间的图可视化地表示为带边的节点。我认为这只是一个不同领域的例子，使用几乎相同的符号来实现不同的目的。

范畴论研究者使用字符串图来证明事物。此外，字符串图用于表示大多数类型的映射，而不仅仅是向量空间之间的映射。更正式地说，字符串图可能出现在讨论任何一类幺半范畴时。为了文雅地介绍这些范畴思想，请看 Fong 和 Spivak 的「Seven Sketches」以及 Coecke 和 Kissinger 的「Picturing Quantum Processes」。

另一方面，一些物理学家和机器学习研究者使用张量网络来计算事物。一个典型的情况可能是这样的。你有一个量子系统。你想找到一个特殊的线性算子的主特征向量，称为哈密顿量。这个特征向量存在于一个大得不可思议的希尔伯特空间中，所以你需要一种技术来以压缩的方式找到这个向量。输入：Tensor Networks。

我所说的「大得不可思议」并不是夸张。如果你有一个阿伏伽德罗数的量子粒子，每个粒子只占据两个状态，那么你需要一个维数为![](../Images/fe14a82f502b803b57f09dbd498bc3ab.jpg)的向量空间。现在想象在这个空间上有一个线性算子。这是一个包含![](../Images/0bab92b187140d98dfc8f8fc877813f2.jpg)个项的矩阵。这比可见宇宙中原子的数目还要多，后者只有 10^80 个！要想把这个矩阵存在电脑上，那么只能祝你好运。总之，张量网络有助于我们以一种原则性的、易于处理的方式处理大量参数。

张量网络也与图模型、自动机等有很多重叠。当前研究的一个脉络是识别并充分利用这些重叠。所以这里有很多东西需要探索。可以从这些地方开始探索：

*   Miles Stoudemire 的 iTensor 库 (http://itensor.org/)：http://itensor.org/

*   Roman Orus 的「A Practical Introduction to Tensor Networks (https://arxiv.org/abs/1306.2164)」：https://arxiv.org/abs/1306.2164

*   Jacob Biamante 和 Ville Bergholm 的「Tensor Networks in a Nutshell」：https://arxiv.org/abs/1708.00006 (https://arxiv.org/abs/1708.00006%E4%BB%A5%E5%8F%8AGoogle%E7%9A%84)

*   Google 的 TensorNetwork 库：https://github.com/google/TensorNetwork

我一直在做一个项目，在一个更具计算性/物理性的环境中使用这些图。因此，我倾向于把它们看作张量网络图，而不是字符串图。

这个项目以一种特殊的张量网络为特色，有一些非常好的数学知识，我很高兴与大家分享。它还使用了之前在博客上讨论过的将矩阵看作图的思想。我计划今年晚些时候在博客上讨论这个问题。****![](../Images/98db554c57db91144fde9866558fb8c3.jpg)****

*原文链接：https://www.math3ma.com/blog/matrices-as-tensor-network-diagrams*

****本文为机器之心编译，**转载请联系本公众号获得授权****。**

✄------------------------------------------------

**加入机器之心（全职记者 / 实习生）：hr@jiqizhixin.com**

**投稿或寻求报道：**content**@jiqizhixin.com**

**广告 & 商务合作：bd@jiqizhixin.com**