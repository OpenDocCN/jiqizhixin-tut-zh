# 新模型学到头秃？gobbli统一模型库帮你快速上手文本分类，内置BERT、fastText等

> 原文：[http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650770496&idx=4&sn=8e0f610bc66ec4deb9495db32d531e0a&chksm=871a483eb06dc128b1166385bf7fdf842742adaef6b49945f26893c1ebf98453037d2e400906&scene=21#wechat_redirect](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650770496&idx=4&sn=8e0f610bc66ec4deb9495db32d531e0a&chksm=871a483eb06dc128b1166385bf7fdf842742adaef6b49945f26893c1ebf98453037d2e400906&scene=21#wechat_redirect)

机器之心报道

**参与：一鸣、张倩**

> 模型太多往往也是个问题，特别是开发者需要逐个学习每个模型的使用方法。最近，RTI International 公司的数据科学家们开发了一个统一的语言模型库 gobbli。用户可以像使用 Keras 那样直接上手文本分类任务，还有很多知名的语言模型可以选择，如 BERT 等。

近年来，迁移学习在自然语言处理领域取得了很大的进展，许多通用语言任务都可以被新提出的模型解决。然而，各种各样的语言模型层出不穷，它们由不同的团队开发，应用这些模型的开发者需要去学习其应用方法，包括与模型进行交互的方法。这无疑提高了开发者使用这些优秀模型的成本。很多使用模型的人往往不是专业的开发者，他们可能是学生、研究机构和大学的研究者或企业雇员。如果能有一个统一的库，让用户从一个渠道去使用各种各样的语言模型，这会为他们带来极大的便利，特别是急需用语言模型却无从下手的社会科学研究者和机器学习初学者。为了解决这一问题，RTI International 公司的数据科学家们开发了一个用于文本分类的语言模型库——gobbli，旨在将自然语言处理和应用领域的最新研究与现实世界的问题和数据关联起来。gobbli 为一般文本分类问题提供了统一接口和一些辅助工具，可以使得迁移学习更加简单，用户可以用简单的代码完成文本分类的任务。![](../Images/ed540f9a1818aa91e07911feedbb8854.jpg)**gobbli 有什么用？**gobbli 是一个开源 python 库，旨在使利用深度学习进行实验和分析更加容易，可以排除由输入/输出格式、库版本等不同而带来的复杂性。gobbli 试图实现一组通用用例，强调可用性而不是性能。gobbli 的目标不是为生产环境提供深度学习模型。每个任务通常都需要在后台运行一个 Docker 容器，并在磁盘之间传输大量数据，这将造成很大的开销。此外，gobbli 也不支持细粒度特定模型调参，如定制损失函数。gobbli 希望能够尽快帮助用户完成 80% 的深度学习解决方案，然后他们再决定是否值得去继续投入精力，解决剩下的 20%。除了为当前最优模型提供统一接口，gobbli 还会提供一些辅助工具，其灵感来自将自然语言处理应用于社会科学和调查研究时常见的问题类型和数据集。

*   项目地址：https://github.com/RTIInternational/gobbli

*   使用文档：https://gobbli.readthedocs.io/en/latest/quickstart.html

**gobbli 怎么用？**gobbli 有着类似于 Keras 的代码风格，非常简洁。用户可利用已有的模型快速实验，也可以先用数据进行训练，然后投入到自己的任务中去使用。此外，gobbli 也支持使用 WordNet 进行数据增强、随机词嵌入等功能，基本上满足了文本分类任务方面的需求。如下为官方文档提供的示例代码： 当用户不需要训练时，只需要导入 gobbli 中的一些包，提供形如列表的数据，就可以开始训练了。甚至标签也不需要换成向量或编号。

```py
from gobbli.experiment import ClassificationExperiment
from gobbli.model import MajorityClassifier
X = ["This is positive.","This is negative.","This is bad.","This is good.","This is really bad.","This is really good.","This is pretty good.","This is pretty bad.",]
y = ["Good","Bad","Bad","Good","Bad","Good","Good","Bad",]
exp = ClassificationExperiment(model_cls=MajorityClassifier,dataset=(X, y))
results = exp.run()
```

gobbli 同时也支持用户进行训练，首先设定训练条件：

```py
from gobbli.io import TrainInput
train_input = TrainInput(
# X_train: A list of strings to classify
X_train=["This is a training document.","This is another training document."],
# y_train: The true class for each string in X_train
y_train=["0", "1"],
# And likewise for validation
X_valid=["This is a validation sentence.","This is another validation sentence."],
y_valid=["1","0"],
# Number of documents to train on at once
train_batch_size=1,
# Number of documents to evaluate at once
valid_batch_size=1,
# Number of times to iterate over the training set
num_train_epochs=1)
```

设置模型：

```py
from gobbli.model import MajorityClassifier
clf = MajorityClassifier()# Set up classifier resources -- Docker image, etc.
clf.build()
```

开始训练：

```py
train_output = clf.train*(train_input)
```

在需要进行推断时：

```py
from gobbli.io import PredictInput
predict_input = PredictInput(
# X: A list of strings to predict the trained classes for
X=["Which class is this document?"],
# Pass the set of labels and the trained checkpoint
# from the training output
labels=train_output.labels,
checkpoint=train_output.checkpoint,
# Number of documents to predict at once
predict_batch_size=1)
predict_output = clf.predict(predict_input)
```

**gobbli 支持哪些模型？**gobbli 目前已经支持很多语言模型，如 BERT、fastText 和简单的 transformer，用户可以根据需要调用。![](../Images/ec62e609798ae689584cc1171be413c8.jpg)**gobbli 怎么安装？**为了使用 gobbli，用户需要安装 Docker，运行 Docker 命令，并安装 Python3.7，然后使用以下命令安装 gobbli 即可：

```py
pip install gobbli
```

*参考链接：**https://www.rti.org/insights/unified-framework-brings-fresh-approach-text-classification*
*https://github.com/RTIInternational/gobbli*********本****文为机器之心报道，**转载请联系本公众号获得授权****。**
✄------------------------------------------------**加入机器之心（全职记者 / 实习生）：hr@jiqizhixin.com****投稿或寻求报道：**content**@jiqizhixin.com****广告 & 商务合作：bd@jiqizhixin.com**