# 模型秒变API只需一行代码，支持TensorFlow等框架

> 原文：[http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650774812&idx=2&sn=bc14f43998f510273ad43fc5cb386923&chksm=871a5962b06dd074077cb100e869f19526d10a549d3f96b3d63e2139d0702df2a172ace0666f&scene=21#wechat_redirect](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650774812&idx=2&sn=bc14f43998f510273ad43fc5cb386923&chksm=871a5962b06dd074077cb100e869f19526d10a549d3f96b3d63e2139d0702df2a172ace0666f&scene=21#wechat_redirect)

选自GitHub

**机器之心编译****参与：****一鸣、杜伟**

> 还在为机器学习模型打包成 API 发愁？这个工具能让你一行代码直接打包。

专注于机器学习应用的人们知道，从训练好的模型到实际的工业生产工具还有一定的距离。其中工作量很大的地方在于将模型打包，预留 API 接口，并和现有的生产系统相结合。近日，GitHub 上有了这样一个项目，能够让用户一行代码将任意模型打包为 API。这一工具无疑能够帮助开发者在实际的生产应用中快速部署模型。项目地址：https://github.com/cortexlabs/cortex**项目特点和原理**该项目名为 Cortex，是一个命令行工具。作者表示，该项目具有以下优点：

*   自动定义：Cortex 可以自动定义需要负责生产工作的 API；

*   多框架支持：Cortex 支持多种机器学习框架，包括 TensorFlow、PyTorch、scikit-learn、XGBoost 等；

*   CPU/GPU 支持：Cortex 能够在 CPU 或者 GPU 上进行推理工作；

*   回滚式更新：Cortex 可以对部署的 API 直接更新；

*   日志流：Cortex 会保留部署模型的日志流，并在 CLI 上显示；

*   预测监控：Cortex 能够监控网络的评价指标，并追踪预测结果；

*   最小配置：部署时，用户只需要在一个名为 cortex.yaml 的文件中配置相关属性。

![](../Images/0ce00001b5f3e24c5c5a6bee0f1f3231.jpg)

这一项目是怎样工作的？具体而言，每当用户运行 cortex deploy 时，命令行将配置属性和代码发送到服务器集群上。每个模型都载入到一个 Docker 容器中，包括相关的 Python 包和处理请求的代码。模型通过网络服务，如 Elastic Load Balancing (ELB)、Flask、TensorFlow Serving 和 ONNX Runtime 公开 API 给用户使用。容器通过 Elastic Kubernetes Service (EKS) 进行控制，而日志文件和评价指标的记录和打印工作由 CloudWatch 完成。**使用方法**使用过程主要分为以下三个步骤：**定义模型的 API**

```py
model = download_my_model()def predict(sample, metadata):            return model.predict(sample["text"])
```

如上所示，用户需要做的是定义代表这个 API 的函数，使其能够根据输入数据返回输出。**配置部署**

```py
- kind: deploymentname: sentiment- kind: apiname: classifierpredictor: path: predictor.pytracker: model_type: classificationcompute: gpu: 1
```

第二个步骤中，用户需要创建一个新的 yaml 文件，这个文件用于配置相关属性。具体而言，用户可以定义部署模型的名称，本例中名为 classifierpredictor。然后还需要定义 API 的名称，如 classifierpredictor 以及路径、模型的类型和使用的 GPU 数量等。**AWS 部署**

```py
$ cortex deploycreating classifier (http://***.amazonaws.com/sentiment/classifier)
```

以 AWS 为例，以上工作完成后，用户可以创建这个 API，使其利用 AWS 进行工作。当然，用户还可以实时保存推断结果，如下所示：

```py
$ curl http://***.amazonaws.com/sentiment/classifier \-d '{"text": "the movie was great!"}'positive
```

此外，用户还可以监控运行结果。

```py
$ cortex get classifier --watchstatus up-to-date available requested last update avg latencyclass countpositive 8negative 4
```

**使用教程**为了让用户更好地使用这一工具，项目作者同时还提供了一些使用案例。包括：

*   基于 TensorFlow 和 BERT 进行情感分析：https://github.com/cortexlabs/cortex/tree/0.10/examples/tensorflow/sentiment-analysis

*   基于 TensorFlow 和 Inception 模型进行图像分类：https://github.com/cortexlabs/cortex/tree/0.10/examples/tensorflow/image-classifier

*   基于 PyTorch 和 DistilGPT2 进行文本生成：https://github.com/cortexlabs/cortex/tree/0.10/examples/pytorch/text-generator

*   基于 XGBoost / ONNX 进行虹膜分类：https://github.com/cortexlabs/cortex/tree/0.10/examples/xgboost/iris-classifier

以使用 BERT 进行情感分析为例：首先用户需要在模型上定义 API 接口函数，使其可以通过函数输入数据，并返回模型的推理结果。这一 py 文件被定义为 handler.py：

```py
import tensorflow as tfimport tensorflow_hub as hubfrom bert import tokenization, run_classifierlabels = ["negative", "positive"]with tf.Graph().as_default(): bert_module = hub.Module("https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1") info = bert_module(signature="tokenization_info", as_dict=True)with tf.Session() as sess: vocab_file, do_lower_case = sess.run([info["vocab_file"], info["do_lower_case"]]) tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)def pre_inference(sample, signature, metadata): input_example = run_classifier.InputExample(guid="", text_a=sample["review"], label=0) input_feature = run_classifier.convert_single_example(0, input_example, [0, 1], 128, tokenizer) return {"input_ids": [input_feature.input_ids]}def post_inference(prediction, signature, metadata): return labels[prediction["labels"][0]]
```

接着，用户需要定义配置 yaml 文件，在文件中指定相关属性，这里需要注意，文件名必须定义为 cortex.yaml：

```py
- kind: deployment name: sentiment- kind: api name: classifier tensorflow: model: s3://cortex-examples/tensorflow/sentiment-analysis/bert request_handler: handler.py tracker: model_type: classification
```

从中可以看到，yaml 文件中需要指定出使用的模型，以及作为 API 接口的 py 文件（即第一步中定义了的 py 文件）。然后进行部署即可：

```py
$ cortex deploydeployment started
```

如果需要获得监控信息，则需要输入 cortex get 命令：

```py
$ cortex get classifier --watchstatus up-to-date available requested last update avg latencylive 1 1 1 8s
```

还可以用命令行获得实时预测：

```py
$ cortex get classifierurl: http://***.amazonaws.com/sentiment/classifier$ curl http://***.amazonaws.com/sentiment/classifier \ -X POST -H "Content-Type: application/json" \ -d '{"review": "The movie was great!"}'"positive
```

第三届机器之心**「Synced Machine Intelligence Awards」**年度奖项评选正在进行中。本次评选设置**六大奖项****，**重点关注人工智能公司的产品、应用案例和产业落地情况，基于真实客观的产业表现筛选出最值得关注的企业，为行业带来实际的参考价值。

**参选报名日期**：2019 年 10 月 23 日~2019 年 12 月 15 日**评审期**：2019 年 12 月 16 日~2019 年 12 月 31 日**奖项公布**：2020 年 1 月

[![](../Images/fb3879a5fda03d04cdebfa8fe8d894a4.jpg)](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650772433&idx=1&sn=64fad90bc878d9f39ced4aca847e9b0e&scene=21#wechat_redirect)

点击**阅读原文**，查看更多详情并报名参选。